{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sankarvinayak/DL-assignment-3/blob/main/DL_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DA6401 Assignment 3\n",
        "Use recurrent neural networks to build a transliteration system."
      ],
      "metadata": {
        "id": "9iyO4eo7ajS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "- The goal of this assignment is fourfold: (i) learn how to model sequence to sequence learning problems using Recurrent Neural Networks (ii) compare different cells such as vanilla RNN, LSTM and GRU (iii) understand how attention networks overcome the limitations of vanilla seq2seq models (iv) visualise the interactions between different components in a RNN based model.\n",
        "- We strongly recommend that you work on this assignment in a team of size 2. Both the members\n",
        "of the team are expected to work together (in a subsequent viva both members will be expected to answer questions, explain the code, etc).\n",
        "- Collaborations and discussions with other groups are strictly prohibited.\n",
        "- You must use Python (numpy and pandas) for your implementation.\n",
        "- You can use any and all packages from keras, pytorch, tensorflow\n",
        "- You can run the code in a jupyter notebook on colab by enabling GPUs.\n",
        "- You have to generate the report in the same format as shown below using wandb.ai. You can start by cloning this report using the clone option above. Most of the plots that we have asked for below can be (automatically) generated using the apis provided by wandb.ai. You will upload a link to this report on gradescope.\n",
        "- You also need to provide a link to your github code as shown below. Follow good software engineering practices and set up a github repo for the project on Day 1. Please do not write all code on your local machine and push everything to github on the last day. The commits in github should reflect how the code has evolved during the course of the assignment.\n",
        "- You have to check moodle regularly for updates regarding the assignment.\n"
      ],
      "metadata": {
        "id": "qgw6bhgEaljz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Problem Statement\n",
        "\n",
        "In this assignment you will experiment with the [Dakshina dataset](https://github.com/google-research-datasets/dakshina) released by Google. This dataset contains pairs of the following form:\n",
        "\n",
        "$x$.      $y$\n",
        "\n",
        "ajanabee अजनबी.\n",
        "\n",
        "i.e., a word in the native script and its corresponding transliteration in the Latin script (the way we type while chatting with our friends on WhatsApp etc). Given many such $(x_i, y_i)_{i=1}^n$ pairs your goal is to train a model $y = \\hat{f}(x)$ which takes as input a romanized string (ghar) and produces the corresponding word in Devanagari (घर).\n",
        "\n",
        "As you would realise this is the problem of mapping a sequence of characters in one language to a sequence of characters in another language. Notice that this is a scaled down version of the problem of translation where the goal is to translate a sequence of **words** in one language to a sequence of words in another language (as opposed to sequence of **characters** here).\n",
        "\n",
        "Read these blogs to understand how to build neural sequence to sequence models: [blog1](https://keras.io/examples/nlp/lstm_seq2seq/), [blog2](https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oZy-iPloatY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xf dakshina_dataset_v1.0.tar\n",
        "!cp -r dakshina_dataset_v1.0/ml .\n",
        "!rm -rf dakshina_dataset_v1.0\n",
        "!rm -r dakshina_dataset_v1.0.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onPyTUOtbfIk",
        "outputId": "a55a2fa4-bc12-48a1-8086-48d44141247a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-15 04:14:44--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.170.207, 173.194.174.207, 74.125.23.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.170.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G  30.5MB/s    in 66s     \n",
            "\n",
            "2025-05-15 04:15:50 (29.2 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths to your data files (adjust if needed)\n",
        "path = \"/content/ml/lexicons/ml.translit.sampled.train.tsv\"\n",
        "path_val = \"/content/ml/lexicons/ml.translit.sampled.dev.tsv\"\n",
        "path_test = \"/content/ml/lexicons/ml.translit.sampled.test.tsv\"\n",
        "\n",
        "# Read the files\n",
        "df = pd.read_csv(path, sep='\\t', header=None)\n",
        "df_val = pd.read_csv(path_val, sep='\\t', header=None)\n",
        "df_test = pd.read_csv(path_test, sep='\\t', header=None)\n",
        "\n",
        "# Split into native (Malayalam) and romanized (English) columns\n",
        "malayalam_words = df[0]\n",
        "english_words = df[1]\n",
        "\n",
        "malayalam_words_val = df_val[0]\n",
        "english_words_val = df_val[1]\n",
        "\n",
        "malayalam_words_test = df_test[0]\n",
        "english_words_test = df_test[1]\n",
        "\n"
      ],
      "metadata": {
        "id": "BQlWAo4vJqKe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "malayalam_words,english_words"
      ],
      "metadata": {
        "id": "MktNed8tJ1cN",
        "outputId": "a2d23eb4-d963-42fb-da82-92aab667d8ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0             അം\n",
              " 1            അംഗ\n",
              " 2            അംഗ\n",
              " 3           അംഗം\n",
              " 4           അംഗം\n",
              "           ...   \n",
              " 58377       ഹൗസ്\n",
              " 58378       ഹർജി\n",
              " 58379       ഹർജി\n",
              " 58380    ഹർജിയിൽ\n",
              " 58381    ഹർജിയിൽ\n",
              " Name: 0, Length: 58382, dtype: object,\n",
              " 0              am\n",
              " 1            amga\n",
              " 2            anga\n",
              " 3           amgam\n",
              " 4           angam\n",
              "            ...   \n",
              " 58377       house\n",
              " 58378       harje\n",
              " 58379       harji\n",
              " 58380    harjeyil\n",
              " 58381    harjiyil\n",
              " Name: 1, Length: 58382, dtype: object)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "english_words = english_words.dropna()\n",
        "malayalam_words = malayalam_words.dropna()\n",
        "\n",
        "english_words = english_words.astype(str)\n",
        "malayalam_words = malayalam_words.astype(str)\n",
        "\n",
        "english_chars = sorted(set(\"\".join(english_words)))\n",
        "malayalam_chars = sorted(set(\"\".join(malayalam_words)))\n",
        "\n",
        "max_len_eng = max(len(w) for w in pd.concat([english_words, english_words_val, english_words_test]).dropna().astype(str))\n",
        "max_len_mal = max(len(w) for w in pd.concat([malayalam_words, malayalam_words_val, malayalam_words_test]).dropna().astype(str))\n",
        "\n",
        "print(\"English characters:\", english_chars)\n",
        "print(\"Malayalam characters:\", malayalam_chars)\n",
        "print(\"Max English word length:\", max_len_eng)\n",
        "print(\"Max Malayalam word length:\", max_len_mal)\n"
      ],
      "metadata": {
        "id": "u0bc09swKX8T",
        "outputId": "d76885d0-1ff4-47aa-e69c-af97f899cdba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English characters: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "Malayalam characters: ['ം', 'ഃ', 'അ', 'ആ', 'ഇ', 'ഈ', 'ഉ', 'ഊ', 'ഋ', 'എ', 'ഏ', 'ഐ', 'ഒ', 'ഓ', 'ഔ', 'ക', 'ഖ', 'ഗ', 'ഘ', 'ങ', 'ച', 'ഛ', 'ജ', 'ഝ', 'ഞ', 'ട', 'ഠ', 'ഡ', 'ഢ', 'ണ', 'ത', 'ഥ', 'ദ', 'ധ', 'ന', 'പ', 'ഫ', 'ബ', 'ഭ', 'മ', 'യ', 'ര', 'റ', 'ല', 'ള', 'ഴ', 'വ', 'ശ', 'ഷ', 'സ', 'ഹ', 'ാ', 'ി', 'ീ', 'ു', 'ൂ', 'ൃ', 'െ', 'േ', 'ൈ', 'ൊ', 'ോ', '്', 'ൗ', 'ൺ', 'ൻ', 'ർ', 'ൽ', 'ൾ', '\\u200c']\n",
            "Max English word length: 32\n",
            "Max Malayalam word length: 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "longest_malayalam_word = max(malayalam_words, key=len)\n",
        "print(\"Longest Malayalam word:\", longest_malayalam_word)\n",
        "print(\"Length:\", len(longest_malayalam_word))\n"
      ],
      "metadata": {
        "id": "Ja9VgGrQMHRr",
        "outputId": "506772df-bb84-477a-a5ab-af4cc7e3fbed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longest Malayalam word: ചൂണ്ടിക്കാണിക്കപ്പെട്ടിട്ടുണ്ട്\n",
            "Length: 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word2vec(word, lang):\n",
        "    vec = []\n",
        "\n",
        "    if lang == \"english\":\n",
        "        start_token = len(english_chars) + 1\n",
        "        vec.append(start_token)\n",
        "\n",
        "        for char in word:\n",
        "            if char in english_chars:\n",
        "                vec.append(english_chars.index(char) + 1)\n",
        "\n",
        "        while len(vec) < max_len_eng + 1:  # +1 for start token\n",
        "            vec.append(0)\n",
        "\n",
        "        vec.append(0)  # end token\n",
        "\n",
        "    elif lang == \"malayalam\":\n",
        "        start_token = len(malayalam_chars) + 1\n",
        "        vec.append(start_token)\n",
        "\n",
        "        for char in word:\n",
        "            if char in malayalam_chars:\n",
        "                vec.append(malayalam_chars.index(char) + 1)\n",
        "\n",
        "        while len(vec) < max_len_mal + 1:\n",
        "            vec.append(0)\n",
        "\n",
        "        vec.append(0)\n",
        "\n",
        "    return vec\n"
      ],
      "metadata": {
        "id": "5KyfyGVTM7rg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec = word2vec(malayalam_words[10], \"malayalam\")\n",
        "print(\"Malayalam word:\", malayalam_words[50000])\n",
        "print(\"Tokenized vector:\", vec)\n"
      ],
      "metadata": {
        "id": "wuRx08ZcNFyy",
        "outputId": "d4920a67-f286-4a0c-be29-c782923be970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Malayalam word: വീണ്\n",
            "Tokenized vector: [71, 3, 1, 18, 20, 63, 20, 45, 52, 41, 53, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ip_matrix_construct(words, lang):\n",
        "    ans = []\n",
        "    for word in words:\n",
        "        ans.append(word2vec(word, lang))\n",
        "    return ans\n"
      ],
      "metadata": {
        "id": "OUKQo2rKOJwO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "english_matrix = ip_matrix_construct(english_words.dropna().astype(str), \"english\")\n",
        "malayalam_matrix = ip_matrix_construct(malayalam_words.dropna().astype(str), \"malayalam\")\n",
        "\n",
        "# Convert to tensors\n",
        "english_matrix = torch.tensor(english_matrix)\n",
        "malayalam_matrix = torch.tensor(malayalam_matrix)\n",
        "\n",
        "# For validation data\n",
        "english_matrix_val = ip_matrix_construct(english_words_val.dropna().astype(str), \"english\")\n",
        "malayalam_matrix_val = ip_matrix_construct(malayalam_words_val.dropna().astype(str), \"malayalam\")\n",
        "english_matrix_val = torch.tensor(english_matrix_val)\n",
        "malayalam_matrix_val = torch.tensor(malayalam_matrix_val)\n",
        "\n",
        "# For test data\n",
        "english_matrix_test = ip_matrix_construct(english_words_test.dropna().astype(str), \"english\")\n",
        "malayalam_matrix_test = ip_matrix_construct(malayalam_words_test.dropna().astype(str), \"malayalam\")\n",
        "english_matrix_test = torch.tensor(english_matrix_test)\n",
        "malayalam_matrix_test = torch.tensor(malayalam_matrix_test)\n"
      ],
      "metadata": {
        "id": "-CjlylkvOM39"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Question 1 (15 Marks)\n",
        "Build a RNN based seq2seq model which contains the following layers: (i) input layer for character embeddings (ii) one encoder RNN which sequentially encodes the input character sequence (Latin) (iii) one decoder RNN which takes the last state of the encoder as input and produces one output character at a time (Devanagari).\n",
        "\n",
        "The code should be flexible such that the dimension of the input character embeddings, the hidden states of the encoders and decoders, the cell (RNN, LSTM, GRU) and the number of layers in the encoder and decoder can be changed.\n",
        "\n",
        "(a) What is the total number of computations done by your network? (assume that the input embedding size is $m$, encoder and decoder have 1 layer each, the hidden cell state is $k$ for both the encoder and decoder, the length of the input and output sequence is the same, i.e., $T$, the size of the vocabulary is the same for the source and target language, i.e., $V$)\n",
        "\n",
        "(b) What is the total number of parameters in your network? (assume that the input embedding size is $m$, encoder and decoder have 1 layer each, the hidden cell state is $k$ for both the encoder and decoder and the length of the input and output sequence is the same, i.e., $T$, the size of the vocabulary is the same for the source and target language, i.e., $V$)\n"
      ],
      "metadata": {
        "id": "6zHBg1GiawdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Question 2 (10 Marks)\n",
        "\n",
        "You will now train your model using any one language from the [Dakshina dataset](https://github.com/google-research-datasets/dakshina) (I would suggest pick a language that you can read so that it is easy to analyse the errors). Use the standard train, dev, test set from the folder dakshina_dataset_v1.0/hi/lexicons/ (replace hi by the language of your choice)\n",
        "\n",
        "Using the sweep feature in wandb find the best hyperparameter configuration. Here are some suggestions but you are free to decide which hyperparameters you want to explore\n",
        "\n",
        "- input embedding size: 16, 32, 64, 256, ...\n",
        "- number of encoder layers: 1, 2, 3\n",
        "- number of decoder layers: 1, 2, 3\n",
        "- hidden layer size: 16, 32, 64, 256, ...\n",
        "- cell type: RNN, GRU, LSTM\n",
        "- dropout: 20%, 30% (btw, where will you add dropout? you should read up a bit on this)\n",
        "- beam search in decoder with different beam sizes:\n",
        "\n",
        "Based on your sweep please paste the following plots which are automatically generated by wandb:\n",
        "- accuracy v/s created plot (I would like to see the number of experiments you ran to get the best configuration).\n",
        "- parallel co-ordinates plot\n",
        "- correlation summary table (to see the correlation of each hyperparameter with the loss/accuracy)\n",
        "\n",
        "Also write down the hyperparameters and their values that you sweeped over. Smart strategies to reduce the number of runs while still achieving a high accuracy would be appreciated. Write down any unique strategy that you tried for efficiently searching the hyperparameters."
      ],
      "metadata": {
        "id": "rcVlhzCxazCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZk8qqrAb9kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
        "  \"parameters\": {\n",
        "    \"embedding_size\": {\"values\": [32, 64, 128]},\n",
        "    \"hidden_size\": {\"values\": [64, 128]},\n",
        "    \"num_encoder_layers\": {\"values\": [1, 2]},\n",
        "    \"num_decoder_layers\": {\"values\": [1, 2]},\n",
        "    \"cell_type\": {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n",
        "    \"dropout\": {\"values\": [0.2, 0.3]},\n",
        "    \"beam_size\": {\"values\": [1, 3, 5]},\n",
        "    \"learning_rate\": {\"values\": [1e-3, 5e-4]}\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "id": "j80CD7Fue-vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"DL-Assignment3\")\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"DL-Assignment3\")\n",
        "wandb.agent(sweep_id, function=train_function)\n"
      ],
      "metadata": {
        "id": "XKc6fQRxe-tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6BbY3Igte-qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OpsREp3Ne-Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bUQyZQqJcWF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Question 3 (15 Marks)\n",
        "Based on the above plots write down some insightful observations. For example,\n",
        "- RNN based model takes longer time to converge than GRU or LSTM\n",
        "- using smaller sizes for the hidden layer does not give good results\n",
        "- dropout leads to better performance\n",
        "\n",
        "(Note: I don't know if any of the above statements is true. I just wrote some random comments that came to my mind)\n",
        "\n",
        "Of course, each inference should be backed by appropriate evidence."
      ],
      "metadata": {
        "id": "4VHpsNBIa100"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F82exrs7cWwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Question 4 (10 Marks)\n",
        "\n",
        "You will now apply your best model on the test data (You shouldn't have used test data so far. All the above experiments should have been done using train and val data only).\n",
        "\n",
        "(a) Use the best model from your sweep and report the accuracy on the test set (the output is correct only if it exactly matches the reference output).\n",
        "\n",
        "(b) Provide sample inputs from the test data and predictions made by your best model (more marks for presenting this grid creatively). Also upload all the predictions on the test set in a folder **predictions_vanilla** on your github project.\n",
        "\n",
        "(c) Comment on the errors made by your model (simple insightful bullet points)\n",
        "\n",
        "- The model makes more errors on consonants than vowels\n",
        "- The model makes more errors on longer sequences\n",
        "- I am thinking confusion matrix but may be it's just me!\n",
        "- ..."
      ],
      "metadata": {
        "id": "xfTCrCc0a495"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r-wFgTO8cXU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Question 5 (20 Marks)\n",
        "\n",
        "Now add an attention network to your basis sequence to sequence model and train the model again. For the sake of simplicity you can use a single layered encoder and a single layered decoder (if you want you can use multiple layers also). Please answer the following questions:\n",
        "\n",
        "(a) Did you tune the hyperparameters again? If yes please paste appropriate plots below.\n",
        "\n",
        "(b) Evaluate your best model on the test set and report the accuracy. Also upload all the predictions on the test set in a folder **predictions_attention** on your github project.\n",
        "\n",
        "(c) Does the attention based model perform better than the vanilla model? If so, can you check some of the errors that this model corrected and note down your inferences (i.e., outputs which were predicted incorrectly by your best seq2seq model are predicted correctly by this model)\n",
        "\n",
        "(d) In a 3 x 3 grid paste the attention heatmaps for 10 inputs from your test data (read up on what are attention heatmaps)."
      ],
      "metadata": {
        "id": "0YRbeDsHa8jp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4ZGQkVWcYZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Question 6 (20 Marks)\n",
        "\n",
        "This a challenge question and most of you will find it hard.\n",
        "\n",
        "I like the visualisation in the figure captioned \"Connectivity\" in this [article](https://distill.pub/2019/memorization-in-rnns/#appendix-autocomplete). Make a similar visualisation for your model. Please look at this [blog](https://towardsdatascience.com/visualising-lstm-activations-in-keras-b50206da96ff) for some starter code. The goal is to figure out the following: When the model is decoding the $i$-th character in the output which is the input character that it is looking at?\n",
        "\n",
        "Have fun!\n"
      ],
      "metadata": {
        "id": "6pUsxVo6a_P2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xmaZ1KnWcY1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Question 7 (10 Marks)\n",
        "Paste a link to your github code for Part A\n",
        "\n",
        "Example: https://github.com/&lt;user-id&gt;/da6401_assignment3/partA;\n",
        "\n",
        "- We will check for coding style, clarity in using functions and a README file with clear instructions on training and evaluating the model (the 10 marks will be based on this).\n",
        "\n",
        "- We will also run a plagiarism check to ensure that the code is not copied (0 marks in the assignment if we find that the code is plagiarised).\n",
        "\n",
        "- We will check the number of commits made by the two team members and then give marks accordingly. For example, if we see 70% of the commits were made by one team member then that member will get more marks in the assignment (**note that this contribution will decide the marks split for the entire assignment and not just this question**).\n",
        "\n",
        "- We will also check if the training and test splits have been used properly. You will get 0 marks on the assignment if we find any cheating (e.g., adding test data to training data) to get higher accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "gSnrEA_0bDJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EGLYAZyJcZcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Question 8 (0 Marks)\n",
        "\n",
        "Note that this question does not carry any marks and will not be graded. This is only for students who are looking for a challenge and want to get something more out of the course.\n",
        "\n",
        "Your task is to finetune the GPT2 model to generate lyrics for English songs. You can refer to [this blog](https://towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a) and follow the steps there. This blog shows how to finetune the GPT2 model to generate headlines for financial articles. Instead of headlines you will use lyrics so you may find the following datasets useful for training: [dataset1](https://data.world/datasets/lyrics), [dataset2](https://www.kaggle.com/paultimothymooney/poetry)\n",
        "\n",
        "At test time you will give it a prompt: \"I love Deep Learning\" and it should complete the song based on this prompt :-) Paste the generated song in a block below!"
      ],
      "metadata": {
        "id": "RvChaPTCbGIP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ccn5krpCcaEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Declaration\n",
        "\n",
        "\n",
        "\n",
        "I, Name_XXX (Roll no: XXYY), swear on my honour that I have written the code and the report by myself and have not copied it from the internet or other students.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HSPz9XJZbNh4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qzf9vWSyaiJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRnIpDOTaHjR"
      },
      "outputs": [],
      "source": []
    }
  ]
}