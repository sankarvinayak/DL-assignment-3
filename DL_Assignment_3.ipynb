{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sankarvinayak/DL-assignment-3/blob/main/DL_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DA6401 Assignment 3\n",
        "Use recurrent neural networks to build a transliteration system."
      ],
      "metadata": {
        "id": "9iyO4eo7ajS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "- The goal of this assignment is fourfold: (i) learn how to model sequence to sequence learning problems using Recurrent Neural Networks (ii) compare different cells such as vanilla RNN, LSTM and GRU (iii) understand how attention networks overcome the limitations of vanilla seq2seq models (iv) visualise the interactions between different components in a RNN based model.\n",
        "- We strongly recommend that you work on this assignment in a team of size 2. Both the members\n",
        "of the team are expected to work together (in a subsequent viva both members will be expected to answer questions, explain the code, etc).\n",
        "- Collaborations and discussions with other groups are strictly prohibited.\n",
        "- You must use Python (numpy and pandas) for your implementation.\n",
        "- You can use any and all packages from keras, pytorch, tensorflow\n",
        "- You can run the code in a jupyter notebook on colab by enabling GPUs.\n",
        "- You have to generate the report in the same format as shown below using wandb.ai. You can start by cloning this report using the clone option above. Most of the plots that we have asked for below can be (automatically) generated using the apis provided by wandb.ai. You will upload a link to this report on gradescope.\n",
        "- You also need to provide a link to your github code as shown below. Follow good software engineering practices and set up a github repo for the project on Day 1. Please do not write all code on your local machine and push everything to github on the last day. The commits in github should reflect how the code has evolved during the course of the assignment.\n",
        "- You have to check moodle regularly for updates regarding the assignment.\n"
      ],
      "metadata": {
        "id": "qgw6bhgEaljz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Problem Statement\n",
        "\n",
        "In this assignment you will experiment with the [Dakshina dataset](https://github.com/google-research-datasets/dakshina) released by Google. This dataset contains pairs of the following form:\n",
        "\n",
        "$x$.      $y$\n",
        "\n",
        "ajanabee अजनबी.\n",
        "\n",
        "i.e., a word in the native script and its corresponding transliteration in the Latin script (the way we type while chatting with our friends on WhatsApp etc). Given many such $(x_i, y_i)_{i=1}^n$ pairs your goal is to train a model $y = \\hat{f}(x)$ which takes as input a romanized string (ghar) and produces the corresponding word in Devanagari (घर).\n",
        "\n",
        "As you would realise this is the problem of mapping a sequence of characters in one language to a sequence of characters in another language. Notice that this is a scaled down version of the problem of translation where the goal is to translate a sequence of **words** in one language to a sequence of words in another language (as opposed to sequence of **characters** here).\n",
        "\n",
        "Read these blogs to understand how to build neural sequence to sequence models: [blog1](https://keras.io/examples/nlp/lstm_seq2seq/), [blog2](https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oZy-iPloatY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xf dakshina_dataset_v1.0.tar\n",
        "!cp -r dakshina_dataset_v1.0/ml .\n",
        "!rm -rf dakshina_dataset_v1.0\n",
        "!rm -r dakshina_dataset_v1.0.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onPyTUOtbfIk",
        "outputId": "991ad643-7759-45a7-a2d5-2c84e7e3c357"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-16 04:26:24--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.145.207, 74.125.128.207, 74.125.143.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.145.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G  40.7MB/s    in 47s     \n",
            "\n",
            "2025-05-16 04:27:12 (40.4 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths to your data files (adjust if needed)\n",
        "path = \"/content/ml/lexicons/ml.translit.sampled.train.tsv\"\n",
        "path_val = \"/content/ml/lexicons/ml.translit.sampled.dev.tsv\"\n",
        "path_test = \"/content/ml/lexicons/ml.translit.sampled.test.tsv\"\n",
        "\n",
        "# Read the files\n",
        "df = pd.read_csv(path, sep='\\t', header=None)\n",
        "df_val = pd.read_csv(path_val, sep='\\t', header=None)\n",
        "df_test = pd.read_csv(path_test, sep='\\t', header=None)\n",
        "\n",
        "# Split into native (Malayalam) and romanized (English) columns\n",
        "malayalam_words = df[0]\n",
        "english_words = df[1]\n",
        "\n",
        "malayalam_words_val = df_val[0]\n",
        "english_words_val = df_val[1]\n",
        "\n",
        "malayalam_words_test = df_test[0]\n",
        "english_words_test = df_test[1]\n",
        "\n"
      ],
      "metadata": {
        "id": "BQlWAo4vJqKe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "malayalam_words,english_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MktNed8tJ1cN",
        "outputId": "dae3bbcc-bdd3-48ea-80bb-7f9f087ca020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0             അം\n",
              " 1            അംഗ\n",
              " 2            അംഗ\n",
              " 3           അംഗം\n",
              " 4           അംഗം\n",
              "           ...   \n",
              " 58377       ഹൗസ്\n",
              " 58378       ഹർജി\n",
              " 58379       ഹർജി\n",
              " 58380    ഹർജിയിൽ\n",
              " 58381    ഹർജിയിൽ\n",
              " Name: 0, Length: 58382, dtype: object,\n",
              " 0              am\n",
              " 1            amga\n",
              " 2            anga\n",
              " 3           amgam\n",
              " 4           angam\n",
              "            ...   \n",
              " 58377       house\n",
              " 58378       harje\n",
              " 58379       harji\n",
              " 58380    harjeyil\n",
              " 58381    harjiyil\n",
              " Name: 1, Length: 58382, dtype: object)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "english_words = english_words.dropna()\n",
        "malayalam_words = malayalam_words.dropna()\n",
        "\n",
        "english_words = english_words.astype(str)\n",
        "malayalam_words = malayalam_words.astype(str)\n",
        "\n",
        "english_chars = sorted(set(\"\".join(english_words)))\n",
        "malayalam_chars = sorted(set(\"\".join(malayalam_words)))\n",
        "\n",
        "max_len_eng = max(len(w) for w in pd.concat([english_words, english_words_val, english_words_test]).dropna().astype(str))\n",
        "max_len_mal = max(len(w) for w in pd.concat([malayalam_words, malayalam_words_val, malayalam_words_test]).dropna().astype(str))\n",
        "\n",
        "print(\"English characters:\", english_chars)\n",
        "print(\"Malayalam characters:\", malayalam_chars)\n",
        "print(\"Max English word length:\", max_len_eng)\n",
        "print(\"Max Malayalam word length:\", max_len_mal)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0bc09swKX8T",
        "outputId": "00a9cc09-704f-44e0-9fc7-99f69bd02eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English characters: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "Malayalam characters: ['ം', 'ഃ', 'അ', 'ആ', 'ഇ', 'ഈ', 'ഉ', 'ഊ', 'ഋ', 'എ', 'ഏ', 'ഐ', 'ഒ', 'ഓ', 'ഔ', 'ക', 'ഖ', 'ഗ', 'ഘ', 'ങ', 'ച', 'ഛ', 'ജ', 'ഝ', 'ഞ', 'ട', 'ഠ', 'ഡ', 'ഢ', 'ണ', 'ത', 'ഥ', 'ദ', 'ധ', 'ന', 'പ', 'ഫ', 'ബ', 'ഭ', 'മ', 'യ', 'ര', 'റ', 'ല', 'ള', 'ഴ', 'വ', 'ശ', 'ഷ', 'സ', 'ഹ', 'ാ', 'ി', 'ീ', 'ു', 'ൂ', 'ൃ', 'െ', 'േ', 'ൈ', 'ൊ', 'ോ', '്', 'ൗ', 'ൺ', 'ൻ', 'ർ', 'ൽ', 'ൾ', '\\u200c']\n",
            "Max English word length: 32\n",
            "Max Malayalam word length: 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "longest_malayalam_word = max(malayalam_words, key=len)\n",
        "print(\"Longest Malayalam word:\", longest_malayalam_word)\n",
        "print(\"Length:\", len(longest_malayalam_word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja9VgGrQMHRr",
        "outputId": "fd663368-77bf-4103-fe08-07c33a13537e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longest Malayalam word: ചൂണ്ടിക്കാണിക്കപ്പെട്ടിട്ടുണ്ട്\n",
            "Length: 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word2vec(word, lang):\n",
        "    vec = []\n",
        "\n",
        "    if lang == \"english\":\n",
        "        start_token = len(english_chars) + 1\n",
        "        vec.append(start_token)\n",
        "\n",
        "        for char in word:\n",
        "            if char in english_chars:\n",
        "                vec.append(english_chars.index(char) + 1)\n",
        "\n",
        "        while len(vec) < max_len_eng + 1:  # +1 for start token\n",
        "            vec.append(0)\n",
        "\n",
        "        vec.append(0)  # end token\n",
        "\n",
        "    elif lang == \"malayalam\":\n",
        "        start_token = len(malayalam_chars) + 1\n",
        "        vec.append(start_token)\n",
        "\n",
        "        for char in word:\n",
        "            if char in malayalam_chars:\n",
        "                vec.append(malayalam_chars.index(char) + 1)\n",
        "\n",
        "        while len(vec) < max_len_mal + 1:\n",
        "            vec.append(0)\n",
        "\n",
        "        vec.append(0)\n",
        "\n",
        "    return vec\n"
      ],
      "metadata": {
        "id": "5KyfyGVTM7rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec = word2vec(malayalam_words[10], \"malayalam\")\n",
        "print(\"Malayalam word:\", malayalam_words[50000])\n",
        "print(\"Tokenized vector:\", vec)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuRx08ZcNFyy",
        "outputId": "b8eb25db-034d-47e3-9cc3-1a656b73310f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Malayalam word: വീണ്\n",
            "Tokenized vector: [71, 3, 1, 18, 20, 63, 20, 45, 52, 41, 53, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ip_matrix_construct(words, lang):\n",
        "    ans = []\n",
        "    for word in words:\n",
        "        ans.append(word2vec(word, lang))\n",
        "    return ans\n"
      ],
      "metadata": {
        "id": "OUKQo2rKOJwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "english_matrix = ip_matrix_construct(english_words.dropna().astype(str), \"english\")\n",
        "malayalam_matrix = ip_matrix_construct(malayalam_words.dropna().astype(str), \"malayalam\")\n",
        "\n",
        "# Convert to tensors\n",
        "english_matrix = torch.tensor(english_matrix)\n",
        "malayalam_matrix = torch.tensor(malayalam_matrix)\n",
        "\n",
        "# For validation data\n",
        "english_matrix_val = ip_matrix_construct(english_words_val.dropna().astype(str), \"english\")\n",
        "malayalam_matrix_val = ip_matrix_construct(malayalam_words_val.dropna().astype(str), \"malayalam\")\n",
        "english_matrix_val = torch.tensor(english_matrix_val)\n",
        "malayalam_matrix_val = torch.tensor(malayalam_matrix_val)\n",
        "\n",
        "# For test data\n",
        "english_matrix_test = ip_matrix_construct(english_words_test.dropna().astype(str), \"english\")\n",
        "malayalam_matrix_test = ip_matrix_construct(malayalam_words_test.dropna().astype(str), \"malayalam\")\n",
        "english_matrix_test = torch.tensor(english_matrix_test)\n",
        "malayalam_matrix_test = torch.tensor(malayalam_matrix_test)\n"
      ],
      "metadata": {
        "id": "-CjlylkvOM39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, src_tensor, trg_tensor):\n",
        "        self.src = src_tensor\n",
        "        self.trg = trg_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src[idx], self.trg[idx]\n",
        "train_dataset = TransliterationDataset(english_matrix, malayalam_matrix)\n",
        "val_dataset = TransliterationDataset(english_matrix_val, malayalam_matrix_val)\n",
        "test_dataset = TransliterationDataset(english_matrix_test, malayalam_matrix_test)\n"
      ],
      "metadata": {
        "id": "amPhoYJASA4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Question 1 (15 Marks)\n",
        "Build a RNN based seq2seq model which contains the following layers: (i) input layer for character embeddings (ii) one encoder RNN which sequentially encodes the input character sequence (Latin) (iii) one decoder RNN which takes the last state of the encoder as input and produces one output character at a time (Devanagari).\n",
        "\n",
        "The code should be flexible such that the dimension of the input character embeddings, the hidden states of the encoders and decoders, the cell (RNN, LSTM, GRU) and the number of layers in the encoder and decoder can be changed.\n",
        "\n",
        "(a) What is the total number of computations done by your network? (assume that the input embedding size is $m$, encoder and decoder have 1 layer each, the hidden cell state is $k$ for both the encoder and decoder, the length of the input and output sequence is the same, i.e., $T$, the size of the vocabulary is the same for the source and target language, i.e., $V$)\n",
        "\n",
        "(b) What is the total number of parameters in your network? (assume that the input embedding size is $m$, encoder and decoder have 1 layer each, the hidden cell state is $k$ for both the encoder and decoder and the length of the input and output sequence is the same, i.e., $T$, the size of the vocabulary is the same for the source and target language, i.e., $V$)\n"
      ],
      "metadata": {
        "id": "6zHBg1GiawdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Question 2 (10 Marks)\n",
        "\n",
        "You will now train your model using any one language from the [Dakshina dataset](https://github.com/google-research-datasets/dakshina) (I would suggest pick a language that you can read so that it is easy to analyse the errors). Use the standard train, dev, test set from the folder dakshina_dataset_v1.0/hi/lexicons/ (replace hi by the language of your choice)\n",
        "\n",
        "Using the sweep feature in wandb find the best hyperparameter configuration. Here are some suggestions but you are free to decide which hyperparameters you want to explore\n",
        "\n",
        "- input embedding size: 16, 32, 64, 256, ...\n",
        "- number of encoder layers: 1, 2, 3\n",
        "- number of decoder layers: 1, 2, 3\n",
        "- hidden layer size: 16, 32, 64, 256, ...\n",
        "- cell type: RNN, GRU, LSTM\n",
        "- dropout: 20%, 30% (btw, where will you add dropout? you should read up a bit on this)\n",
        "- beam search in decoder with different beam sizes:\n",
        "\n",
        "Based on your sweep please paste the following plots which are automatically generated by wandb:\n",
        "- accuracy v/s created plot (I would like to see the number of experiments you ran to get the best configuration).\n",
        "- parallel co-ordinates plot\n",
        "- correlation summary table (to see the correlation of each hyperparameter with the loss/accuracy)\n",
        "\n",
        "Also write down the hyperparameters and their values that you sweeped over. Smart strategies to reduce the number of runs while still achieving a high accuracy would be appreciated. Write down any unique strategy that you tried for efficiently searching the hyperparameters."
      ],
      "metadata": {
        "id": "rcVlhzCxazCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout, cell_type='LSTM', bidirectional=False):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        if cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, bidirectional=bidirectional)\n",
        "        elif cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, bidirectional=bidirectional)\n",
        "        else:  # LSTM by default\n",
        "            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (seq_len, batch)\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        # embedded shape: (seq_len, batch, embedding_size)\n",
        "\n",
        "        if self.cell_type == 'LSTM':\n",
        "            outputs, (hidden, cell) = self.rnn(embedded)\n",
        "            return outputs, (hidden, cell)\n",
        "        else:\n",
        "            outputs, hidden = self.rnn(embedded)\n",
        "            return outputs, hidden\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, embedding_size, hidden_size, num_layers, dropout, cell_type='LSTM'):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        if cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0)\n",
        "        elif cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0)\n",
        "        else:  # LSTM\n",
        "            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0)\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # x shape: (batch), because one timestep input\n",
        "        x = x.unsqueeze(0)  # now (1, batch)\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        # embedded shape: (1, batch, embedding_size)\n",
        "\n",
        "        if self.cell_type == 'LSTM':\n",
        "            output, (hidden, cell) = self.rnn(embedded, hidden)\n",
        "            prediction = self.fc_out(output.squeeze(0))\n",
        "            return prediction, (hidden, cell)\n",
        "        else:\n",
        "            output, hidden = self.rnn(embedded, hidden)\n",
        "            prediction = self.fc_out(output.squeeze(0))\n",
        "            return prediction, hidden\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src shape: (src_len, batch)\n",
        "        # trg shape: (trg_len, batch)\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.fc_out.out_features\n",
        "\n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # encoder forward pass\n",
        "        if self.encoder.cell_type == 'LSTM':\n",
        "            encoder_outputs, (hidden, cell) = self.encoder(src)\n",
        "        else:\n",
        "            encoder_outputs, hidden = self.encoder(src)\n",
        "            cell = None  # no cell in RNN/GRU\n",
        "\n",
        "        # first input to the decoder is the <sos> token\n",
        "        input = trg[0, :]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            if self.encoder.cell_type == 'LSTM':\n",
        "                output, (hidden, cell) = self.decoder(input, (hidden, cell))\n",
        "            else:\n",
        "                output, hidden = self.decoder(input, hidden)\n",
        "\n",
        "            outputs[t] = output\n",
        "\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "    def beam_search(self, src, sos_idx, eos_idx, max_len=30, beam_width=3):\n",
        "        # src: (src_len, batch=1) - typically batch=1 for beam search inference\n",
        "        self.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Encode source sentence\n",
        "            if self.encoder.cell_type == 'LSTM':\n",
        "                encoder_outputs, (hidden, cell) = self.encoder(src)\n",
        "            else:\n",
        "                encoder_outputs, hidden = self.encoder(src)\n",
        "                cell = None\n",
        "\n",
        "            # Initialize beam with sequences, scores, and hidden states\n",
        "            sequences = [[ [sos_idx], 0.0, hidden, cell ]]  # list of [sequence, score, hidden, cell]\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                all_candidates = []\n",
        "                # Expand each sequence in the beam\n",
        "                for seq, score, hidden_state, cell_state in sequences:\n",
        "                    # If last token is EOS, add sequence as is\n",
        "                    if seq[-1] == eos_idx:\n",
        "                        all_candidates.append((seq, score, hidden_state, cell_state))\n",
        "                        continue\n",
        "\n",
        "                    input_token = torch.LongTensor([seq[-1]]).to(self.device)\n",
        "                    if self.encoder.cell_type == 'LSTM':\n",
        "                        output, (hidden_new, cell_new) = self.decoder(input_token, (hidden_state, cell_state))\n",
        "                    else:\n",
        "                        output, hidden_new = self.decoder(input_token, hidden_state)\n",
        "                        cell_new = None\n",
        "\n",
        "                    # Get log probabilities\n",
        "                    log_probs = F.log_softmax(output, dim=1).squeeze(0)  # (vocab_size,)\n",
        "\n",
        "                    # Get top beam_width tokens\n",
        "                    top_log_probs, top_indices = torch.topk(log_probs, beam_width)\n",
        "\n",
        "                    for i in range(beam_width):\n",
        "                        candidate_seq = seq + [top_indices[i].item()]\n",
        "                        candidate_score = score + top_log_probs[i].item()\n",
        "                        all_candidates.append((candidate_seq, candidate_score, hidden_new, cell_new))\n",
        "\n",
        "                # Order all candidates by score and select top beam_width\n",
        "                ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n",
        "                sequences = ordered[:beam_width]\n",
        "\n",
        "                # Optional: break early if all sequences end with EOS\n",
        "                if all(seq[-1] == eos_idx for seq, _, _, _ in sequences):\n",
        "                    break\n",
        "\n",
        "            # Return the highest scoring sequence\n",
        "            best_seq = sequences[0][0]\n",
        "            return best_seq\n"
      ],
      "metadata": {
        "id": "YZk8qqrAb9kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "INPUT_DIM = len(english_chars) + 2  # +2 for padding and possibly <sos>/<eos>\n",
        "OUTPUT_DIM = len(malayalam_chars) + 2\n",
        "\n",
        "ENC_EMB_DIM = 64\n",
        "DEC_EMB_DIM = 64\n",
        "HID_DIM = 128\n",
        "ENC_LAYERS = 2\n",
        "DEC_LAYERS = 2\n",
        "ENC_DROPOUT = 0.3\n",
        "DEC_DROPOUT = 0.3\n",
        "CELL_TYPE ='LSTM' # 'LSTM'  or 'GRU' or 'RNN'\n",
        "\n",
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_LAYERS, ENC_DROPOUT, CELL_TYPE).to(device)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_LAYERS, DEC_DROPOUT, CELL_TYPE).to(device)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "import torch.optim as optim\n",
        "\n",
        "PAD_IDX = 0  # assuming 0 is padding index in your tokenizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # padding token is 0\n",
        "CLIP = 1\n"
      ],
      "metadata": {
        "id": "51sAR-XSQ62P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lu50IEfSKpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for src, trg in iterator:\n",
        "        src = src.transpose(0, 1).to(device)  # (seq_len, batch)\n",
        "        trg = trg.transpose(0, 1).to(device)  # (seq_len, batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg[:-1, :])  # (seq_len-1, batch, output_dim)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.reshape(-1, output_dim)  # ( (seq_len-1)*batch, output_dim )\n",
        "        trg_y = trg[1:, :].reshape(-1)           # ( (seq_len-1)*batch )\n",
        "\n",
        "        loss = criterion(output, trg_y)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = output.argmax(1)                # predicted tokens\n",
        "        non_pad_mask = trg_y != 0               # ignore padding\n",
        "        correct = (preds == trg_y) & non_pad_mask\n",
        "\n",
        "        epoch_acc += correct.sum().item()\n",
        "        total_tokens += non_pad_mask.sum().item()\n",
        "\n",
        "    accuracy = epoch_acc / total_tokens if total_tokens > 0 else 0\n",
        "    return epoch_loss / len(iterator), accuracy\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in iterator:\n",
        "            src = src.transpose(0, 1).to(model.device)  # (seq_len, batch)\n",
        "            trg = trg.transpose(0, 1).to(model.device)\n",
        "\n",
        "            output = model(src, trg, 0)  # turn off teacher forcing\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)  # remove <sos>, flatten\n",
        "            trg_y = trg[1:].reshape(-1)               # remove <sos>, flatten\n",
        "\n",
        "            loss = criterion(output, trg_y)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            preds = output.argmax(1)\n",
        "            non_pad_mask = trg_y != 0\n",
        "            correct = (preds == trg_y) & non_pad_mask\n",
        "\n",
        "            epoch_acc += correct.sum().item()\n",
        "            total_tokens += non_pad_mask.sum().item()\n",
        "\n",
        "    accuracy = epoch_acc / total_tokens if total_tokens > 0 else 0\n",
        "    return epoch_loss / len(iterator), accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "tblXSnyTTiJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, train_acc = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "\n",
        "print(f\"Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2%}\")\n",
        "valid_loss, valid_acc = evaluate(model, val_loader, criterion)\n",
        "\n",
        "print(f\"Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoOw4Ev5TlDd",
        "outputId": "02dc342e-6a1e-4683-e5be-f5fd9feef52f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.756 | Train Acc: 53.05%\n",
            "Val. Loss: 4.165 | Val. Acc: 13.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "INPUT_DIM = len(english_chars) + 2  # +2 for padding and special tokens <sos>/<eos>\n",
        "OUTPUT_DIM = len(malayalam_chars) + 2\n",
        "\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "ENC_LAYERS = 5\n",
        "DEC_LAYERS = 5\n",
        "ENC_DROPOUT = 0.0\n",
        "DEC_DROPOUT = 0.0\n",
        "CELL_TYPE = 'GRU'\n",
        "BIDIRECTIONAL = False\n",
        "BATCH_SIZE = 256\n",
        "CLIP = 1\n",
        "\n",
        "# Create Encoder and Decoder\n",
        "\n",
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_LAYERS, ENC_DROPOUT, CELL_TYPE).to(device)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_LAYERS, DEC_DROPOUT, CELL_TYPE).to(device)\n",
        "# Initialize Seq2Seq model with encoder and decoder\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "import torch.optim as optim\n",
        "\n",
        "PAD_IDX = 0  # assuming 0 is the padding index in your tokenizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "NZ5PGeWGjgMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 19\n",
        "\n",
        "for i in range(epochs):\n",
        "    print(f\"Epoch: {i+1}/{epochs}\")\n",
        "\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    print(f\"Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2%}\")\n",
        "\n",
        "    valid_loss, valid_acc = evaluate(model, val_loader, criterion)\n",
        "    print(f\"Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc:.2%}\")\n"
      ],
      "metadata": {
        "id": "GpOW-0X1jgJo",
        "outputId": "70e46e39-f5cc-44e6-ff31-9bbde0845e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, malayalam_words, english_words, ml_char_to_idx, en_char_to_idx):\n",
        "        self.malayalam_words = malayalam_words\n",
        "        self.english_words = english_words\n",
        "        self.ml_char_to_idx = ml_char_to_idx\n",
        "        self.en_char_to_idx = en_char_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.malayalam_words)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ml_word = self.malayalam_words[idx]\n",
        "        en_word = self.english_words[idx]\n",
        "\n",
        "        # Convert characters to indices\n",
        "        ml_indices = [self.ml_char_to_idx.get(char, self.ml_char_to_idx['<UNK>']) for char in ml_word]\n",
        "        # Add start and end tokens for English\n",
        "        en_indices = [self.en_char_to_idx['<SOS>']] + [self.en_char_to_idx.get(char, self.en_char_to_idx['<UNK>']) for char in en_word] + [self.en_char_to_idx['<EOS>']]\n",
        "\n",
        "        return torch.tensor(ml_indices), torch.tensor(en_indices), len(ml_indices), len(en_indices)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    ml_words, en_words, ml_lengths, en_lengths = zip(*batch)\n",
        "\n",
        "    # Pad sequences\n",
        "    ml_words_padded = pad_sequence(ml_words, batch_first=True, padding_value=0)\n",
        "    en_words_padded = pad_sequence(en_words, batch_first=True, padding_value=0)\n",
        "\n",
        "    return ml_words_padded, en_words_padded, torch.tensor(ml_lengths), torch.tensor(en_lengths)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_dim, hidden_size, num_layers, dropout, cell_type='LSTM'):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        if cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        elif cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        elif cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Cell type must be 'LSTM', 'GRU', or 'RNN'\")\n",
        "\n",
        "    def forward(self, src, src_lengths):\n",
        "        # src: [batch_size, src_len]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded: [batch_size, src_len, embedding_dim]\n",
        "\n",
        "        # Pack padded sequences for more efficient computation\n",
        "        packed_embedded = pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        if self.cell_type == 'LSTM':\n",
        "            packed_outputs, (hidden, cell) = self.rnn(packed_embedded)\n",
        "            outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "            return outputs, (hidden, cell)\n",
        "        else:\n",
        "            packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "            outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "            return outputs, hidden\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: [batch_size, hidden_size]\n",
        "        # encoder_outputs: [batch_size, src_len, hidden_size]\n",
        "\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "\n",
        "        # Repeat decoder hidden state across sequence length\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        # energy: [batch_size, src_len, hidden_size]\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "\n",
        "        # v: [hidden_size] -> [batch_size, src_len]\n",
        "        attention = torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "        # Return attention weights and apply them to encoder outputs\n",
        "        return torch.softmax(attention, dim=1)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, embedding_dim, hidden_size, num_layers, dropout, cell_type='LSTM', attention=False):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # If using attention, we'll concatenate the attention output with embedding\n",
        "        input_size = embedding_dim\n",
        "        if attention:\n",
        "            self.attention_layer = Attention(hidden_size)\n",
        "            self.fc_combine = nn.Linear(hidden_size + embedding_dim, embedding_dim)\n",
        "\n",
        "        if cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        elif cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(input_size, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        elif cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Cell type must be 'LSTM', 'GRU', or 'RNN'\")\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs=None):\n",
        "        # input: [batch_size, 1]\n",
        "        # hidden: tuple of [num_layers, batch_size, hidden_size] (for LSTM)\n",
        "        # encoder_outputs: [batch_size, src_len, hidden_size] (needed for attention)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded: [batch_size, 1, embedding_dim]\n",
        "\n",
        "        if self.attention and encoder_outputs is not None:\n",
        "            # Get the last hidden state from the top layer for attention\n",
        "            if self.cell_type == 'LSTM':\n",
        "                attn_hidden = hidden[0][-1]\n",
        "            else:\n",
        "                attn_hidden = hidden[-1]\n",
        "\n",
        "            # Apply attention\n",
        "            attn_weights = self.attention_layer(attn_hidden, encoder_outputs)\n",
        "            # attn_weights: [batch_size, src_len]\n",
        "\n",
        "            # Apply attention weights to encoder outputs\n",
        "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
        "            # context: [batch_size, 1, hidden_size]\n",
        "\n",
        "            # Combine context and embedded\n",
        "            embedded = self.fc_combine(torch.cat((embedded.squeeze(1), context.squeeze(1)), dim=1)).unsqueeze(1)\n",
        "\n",
        "        # Pass through RNN\n",
        "        if self.cell_type == 'LSTM':\n",
        "            output, (hidden, cell) = self.rnn(embedded, hidden)\n",
        "            output = self.fc_out(output)\n",
        "            return output, (hidden, cell)\n",
        "        else:\n",
        "            output, hidden = self.rnn(embedded, hidden)\n",
        "            output = self.fc_out(output)\n",
        "            return output, hidden\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, src_lengths, trg, teacher_forcing_ratio=0.5):\n",
        "        # src: [batch_size, src_len]\n",
        "        # trg: [batch_size, trg_len]\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_size\n",
        "\n",
        "        # Tensor to store decoder outputs\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # Encode the source sequence\n",
        "        encoder_outputs, hidden = self.encoder(src, src_lengths)\n",
        "\n",
        "        # First input to the decoder is the <SOS> token\n",
        "        input = trg[:, 0:1]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            # Use teacher forcing (use actual target as input) or not\n",
        "            use_teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            # Forward pass through decoder\n",
        "            if self.decoder.attention:\n",
        "                output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            else:\n",
        "                output, hidden = self.decoder(input, hidden)\n",
        "\n",
        "            # output: [batch_size, 1, output_size]\n",
        "            outputs[:, t:t+1, :] = output\n",
        "\n",
        "            # Get the next input for decoder\n",
        "            top1 = output.argmax(2) if not use_teacher_force else trg[:, t:t+1]\n",
        "            input = top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def infer(self, src, src_lengths, max_len=50, beam_size=1):\n",
        "        # For inference with or without beam search\n",
        "        batch_size = src.shape[0]\n",
        "\n",
        "        # Encode the source sequence\n",
        "        encoder_outputs, hidden = self.encoder(src, src_lengths)\n",
        "\n",
        "        if beam_size == 1:\n",
        "            # Simple greedy decoding\n",
        "            return self._greedy_decode(encoder_outputs, hidden, max_len)\n",
        "        else:\n",
        "            # Beam search decoding\n",
        "            return self._beam_search_decode(encoder_outputs, hidden, max_len, beam_size)\n",
        "\n",
        "    def _greedy_decode(self, encoder_outputs, hidden, max_len):\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "\n",
        "        # Start with <SOS> tokens\n",
        "        input = torch.tensor([[1]] * batch_size).to(self.device)  # Assuming 1 is <SOS>\n",
        "\n",
        "        # Lists to store predicted indices\n",
        "        predictions = torch.zeros(batch_size, max_len, dtype=torch.long).to(self.device)\n",
        "\n",
        "        for t in range(max_len):\n",
        "            # Forward pass through decoder\n",
        "            if self.decoder.attention:\n",
        "                output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            else:\n",
        "                output, hidden = self.decoder(input, hidden)\n",
        "\n",
        "            # Get most likely next token\n",
        "            top1 = output.argmax(2)\n",
        "            predictions[:, t] = top1.squeeze()\n",
        "\n",
        "            # Break if all sequences have generated <EOS>\n",
        "            if (top1 == 2).all():  # Assuming 2 is <EOS>\n",
        "                break\n",
        "\n",
        "            # Update input for next timestep\n",
        "            input = top1\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def _beam_search_decode(self, encoder_outputs, hidden, max_len, beam_size):\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "\n",
        "        # List to store final predictions for each item in batch\n",
        "        batch_predictions = []\n",
        "\n",
        "        # Process each item in batch separately for beam search\n",
        "        for b in range(batch_size):\n",
        "            # Get encoder outputs and hidden state for this item\n",
        "            single_encoder_output = encoder_outputs[b:b+1]\n",
        "\n",
        "            if self.decoder.cell_type == 'LSTM':\n",
        "                single_hidden = (hidden[0][:, b:b+1, :], hidden[1][:, b:b+1, :])\n",
        "            else:\n",
        "                single_hidden = hidden[:, b:b+1, :]\n",
        "\n",
        "            # Start with <SOS> token\n",
        "            input = torch.tensor([[1]]).to(self.device)  # Assuming 1 is <SOS>\n",
        "\n",
        "            # Lists to keep track of beams: (sequence, score, hidden_state)\n",
        "            beams = [(torch.tensor([[1]], device=self.device), 0, single_hidden)]\n",
        "            complete_beams = []\n",
        "\n",
        "            for t in range(max_len):\n",
        "                new_beams = []\n",
        "\n",
        "                # Explore each current beam\n",
        "                for sequence, score, beam_hidden in beams:\n",
        "                    # Skip completed sequences\n",
        "                    if sequence[0, -1].item() == 2:  # <EOS> token\n",
        "                        complete_beams.append((sequence, score, beam_hidden))\n",
        "                        continue\n",
        "\n",
        "                    # Use last token as input\n",
        "                    beam_input = sequence[:, -1:].to(self.device)\n",
        "\n",
        "                    # Forward pass through decoder\n",
        "                    if self.decoder.attention:\n",
        "                        output, new_hidden = self.decoder(beam_input, beam_hidden, single_encoder_output)\n",
        "                    else:\n",
        "                        output, new_hidden = self.decoder(beam_input, beam_hidden)\n",
        "\n",
        "                    # Get log probabilities\n",
        "                    log_probs = nn.functional.log_softmax(output.squeeze(1), dim=1)\n",
        "\n",
        "                    # Get top beam_size probabilities\n",
        "                    topk_probs, topk_idx = log_probs.topk(beam_size)\n",
        "\n",
        "                    # Create new beams\n",
        "                    for i in range(beam_size):\n",
        "                        token = topk_idx[0, i].unsqueeze(0).unsqueeze(0)\n",
        "                        new_seq = torch.cat([sequence, token], dim=1)\n",
        "                        new_score = score + topk_probs[0, i].item()\n",
        "                        new_beams.append((new_seq, new_score, new_hidden))\n",
        "\n",
        "                # Keep only the top beam_size beams\n",
        "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
        "\n",
        "                # Stop if all beams end with <EOS>\n",
        "                if all(beam[0][0, -1].item() == 2 for beam in beams):\n",
        "                    complete_beams.extend(beams)\n",
        "                    break\n",
        "\n",
        "            # Handle case if no complete beams (no <EOS> found)\n",
        "            if not complete_beams:\n",
        "                complete_beams = beams\n",
        "\n",
        "            # Sort completed beams by score and return highest scoring sequence\n",
        "            best_beam = max(complete_beams, key=lambda x: x[1])\n",
        "            batch_predictions.append(best_beam[0])\n",
        "\n",
        "        # Pad sequences to same length for batch\n",
        "        max_pred_len = max(pred.shape[1] for pred in batch_predictions)\n",
        "        padded_preds = torch.zeros(batch_size, max_pred_len, dtype=torch.long).to(self.device)\n",
        "\n",
        "        for i, pred in enumerate(batch_predictions):\n",
        "            padded_preds[i, :pred.shape[1]] = pred\n",
        "\n",
        "        return padded_preds\n",
        "\n",
        "\n",
        "def create_vocabularies(malayalam_words, english_words):\n",
        "    # Create Malayalam character vocabulary\n",
        "    ml_chars = set()\n",
        "    for word in malayalam_words:\n",
        "        ml_chars.update(word)\n",
        "\n",
        "    # Create English character vocabulary\n",
        "    en_chars = set()\n",
        "    for word in english_words:\n",
        "        en_chars.update(word)\n",
        "\n",
        "    # Add special tokens\n",
        "    ml_char_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    en_char_to_idx = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
        "\n",
        "    # Add characters to dictionaries\n",
        "    for i, char in enumerate(sorted(ml_chars)):\n",
        "        ml_char_to_idx[char] = i + 2  # +2 for <PAD> and <UNK>\n",
        "\n",
        "    for i, char in enumerate(sorted(en_chars)):\n",
        "        en_char_to_idx[char] = i + 4  # +4 for <PAD>, <UNK>, <SOS>, <EOS>\n",
        "\n",
        "    ml_idx_to_char = {idx: char for char, idx in ml_char_to_idx.items()}\n",
        "    en_idx_to_char = {idx: char for char, idx in en_char_to_idx.items()}\n",
        "\n",
        "    return ml_char_to_idx, en_char_to_idx, ml_idx_to_char, en_idx_to_char\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            src, trg, src_lengths, trg_lengths = batch\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            output = model(src, src_lengths, trg, teacher_forcing_ratio=0)\n",
        "\n",
        "            # Reshape for computing loss\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(output, trg)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, optimizer, criterion, n_epochs, device, clip=1, teacher_forcing_ratio=0.5):\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{n_epochs}'):\n",
        "            src, trg, src_lengths, trg_lengths = batch\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(src, src_lengths, trg, teacher_forcing_ratio)\n",
        "\n",
        "            # Reshape for computing loss\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_loss = evaluate(model, val_dataloader, criterion, device)\n",
        "\n",
        "        # Save losses for plotting\n",
        "        train_losses.append(epoch_loss / len(train_dataloader))\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02}')\n",
        "        print(f'\\tTrain Loss: {epoch_loss / len(train_dataloader):.4f}')\n",
        "        print(f'\\tValidation Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_transliteration_model.pt')\n",
        "            print(f'\\tBest model saved with validation loss: {val_loss:.4f}')\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "def predict(model, src, src_lengths, en_idx_to_char, device, beam_size=1):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Forward pass with beam search or greedy decoding\n",
        "        predictions = model.infer(src.to(device), src_lengths, beam_size=beam_size)\n",
        "\n",
        "    # Convert predictions to characters\n",
        "    predicted_words = []\n",
        "    for pred in predictions:\n",
        "        word = []\n",
        "        for idx in pred:\n",
        "            idx = idx.item()\n",
        "            if idx == 3:  # <EOS> token\n",
        "                break\n",
        "            if idx > 3:  # Skip special tokens\n",
        "                word.append(en_idx_to_char[idx])\n",
        "        predicted_words.append(''.join(word))\n",
        "\n",
        "    return predicted_words\n",
        "\n",
        "\n",
        "def calculate_accuracy(predicted_words, target_words):\n",
        "    correct = 0\n",
        "    for pred, target in zip(predicted_words, target_words):\n",
        "        if pred == target:\n",
        "            correct += 1\n",
        "    return correct / len(target_words) * 100.0\n",
        "\n",
        "\n",
        "def plot_learning_curves(train_losses, val_losses):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('learning_curves.png')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Malayalam to English Transliteration')\n",
        "    parser.add_argument('--embedding_dim', type=int, default=64, choices=[16, 32, 64, 256],\n",
        "                        help='Embedding dimension size')\n",
        "    parser.add_argument('--encoder_layers', type=int, default=2, choices=[1, 2, 3],\n",
        "                        help='Number of encoder layers')\n",
        "    parser.add_argument('--decoder_layers', type=int, default=2, choices=[1, 2, 3],\n",
        "                        help='Number of decoder layers')\n",
        "    parser.add_argument('--hidden_size', type=int, default=64, choices=[16, 32, 64, 256],\n",
        "                        help='Hidden layer size')\n",
        "    parser.add_argument('--cell_type', type=str, default='LSTM', choices=['RNN', 'GRU', 'LSTM'],\n",
        "                        help='RNN cell type')\n",
        "    parser.add_argument('--dropout', type=float, default=0.2, choices=[0.2, 0.3],\n",
        "                        help='Dropout rate')\n",
        "    parser.add_argument('--attention', action='store_true',\n",
        "                        help='Use attention mechanism')\n",
        "    parser.add_argument('--teacher_forcing', type=float, default=0.5,\n",
        "                        help='Teacher forcing ratio (0 to disable)')\n",
        "    parser.add_argument('--beam_size', type=int, default=1,\n",
        "                        help='Beam size for beam search decoding (1 for greedy)')\n",
        "    parser.add_argument('--batch_size', type=int, default=64,\n",
        "                        help='Batch size for training')\n",
        "    parser.add_argument('--epochs', type=int, default=15,\n",
        "                        help='Number of training epochs')\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.001,\n",
        "                        help='Learning rate')\n",
        "    parser.add_argument('--data_path', type=str, default='/content/ml/lexicons/ml.translit.sampled.train.tsv',\n",
        "                        help='Path to training data')\n",
        "    parser.add_argument('--val_path', type=str, default='/content/ml/lexicons/ml.translit.sampled.dev.tsv',\n",
        "                        help='Path to validation data')\n",
        "    parser.add_argument('--test_path', type=str, default='/content/ml/lexicons/ml.translit.sampled.test.tsv',\n",
        "                        help='Path to test data')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    try:\n",
        "        df = pd.read_csv(args.data_path, sep='\\t', header=None)\n",
        "        df_val = pd.read_csv(args.val_path, sep='\\t', header=None)\n",
        "        df_test = pd.read_csv(args.test_path, sep='\\t', header=None)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: File not found. Please check file paths.\")\n",
        "        return\n",
        "\n",
        "    # Split into native (Malayalam) and romanized (English) columns\n",
        "    malayalam_words = df[0].tolist()\n",
        "    english_words = df[1].tolist()\n",
        "\n",
        "    malayalam_words_val = df_val[0].tolist()\n",
        "    english_words_val = df_val[1].tolist()\n",
        "\n",
        "    malayalam_words_test = df_test[0].tolist()\n",
        "    english_words_test = df_test[1].tolist()\n",
        "\n",
        "    # Create vocabularies\n",
        "    ml_char_to_idx, en_char_to_idx, ml_idx_to_char, en_idx_to_char = create_vocabularies(\n",
        "        malayalam_words + malayalam_words_val + malayalam_words_test,\n",
        "        english_words + english_words_val + english_words_test\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TransliterationDataset(malayalam_words, english_words, ml_char_to_idx, en_char_to_idx)\n",
        "    val_dataset = TransliterationDataset(malayalam_words_val, english_words_val, ml_char_to_idx, en_char_to_idx)\n",
        "    test_dataset = TransliterationDataset(malayalam_words_test, english_words_test, ml_char_to_idx, en_char_to_idx)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Create model\n",
        "    input_size = len(ml_char_to_idx)\n",
        "    output_size = len(en_char_to_idx)\n",
        "\n",
        "    encoder = Encoder(\n",
        "        input_size=input_size,\n",
        "        embedding_dim=args.embedding_dim,\n",
        "        hidden_size=args.hidden_size,\n",
        "        num_layers=args.encoder_layers,\n",
        "        dropout=args.dropout,\n",
        "        cell_type=args.cell_type\n",
        "    )\n",
        "\n",
        "    decoder = Decoder(\n",
        "        output_size=output_size,\n",
        "        embedding_dim=args.embedding_dim,\n",
        "        hidden_size=args.hidden_size,\n",
        "        num_layers=args.decoder_layers,\n",
        "        dropout=args.dropout,\n",
        "        cell_type=args.cell_type,\n",
        "        attention=args.attention\n",
        "    )\n",
        "\n",
        "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
        "\n",
        "    # Print model architecture and hyperparameters\n",
        "    print(f\"Model Architecture:\")\n",
        "    print(f\"Input Size: {input_size}\")\n",
        "    print(f\"Output Size: {output_size}\")\n",
        "    print(f\"Embedding Dimension: {args.embedding_dim}\")\n",
        "    print(f\"Hidden Size: {args.hidden_size}\")\n",
        "    print(f\"Encoder Layers: {args.encoder_layers}\")\n",
        "    print(f\"Decoder Layers: {args.decoder_layers}\")\n",
        "    print(f\"Cell Type: {args.cell_type}\")\n",
        "    print(f\"Dropout: {args.dropout}\")\n",
        "    print(f\"Attention: {args.attention}\")\n",
        "    print(f\"Teacher Forcing Ratio: {args.teacher_forcing}\")\n",
        "    print(f\"Beam Size: {args.beam_size}\")\n",
        "    print(f\"Batch Size: {args.batch_size}\")\n",
        "    print(f\"Learning Rate: {args.learning_rate}\")\n",
        "    print(f\"Number of Epochs: {args.epochs}\")\n",
        "\n",
        "    # Train model\n",
        "    start_time = time.time()\n",
        "    train_losses, val_losses = train(\n",
        "        model,\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        args.epochs,\n",
        "        device,\n",
        "        teacher_forcing_ratio=args.teacher_forcing\n",
        "    )\n",
        "    print(f\"Training completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # Plot learning curves\n",
        "    plot_learning_curves(train_losses, val_losses)\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load('best_transliteration_model.pt'))\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss = evaluate(model, test_dataloader, criterion, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Test some examples with and without beam search\n",
        "    num_examples = min(5, len(test_dataset))\n",
        "    for i in range(num_examples):\n",
        "        src, trg, src_len, trg_len = test_dataset[i]\n",
        "        src = src.unsqueeze(0)  # Add batch dimension\n",
        "        src_len = torch.tensor([src_len])\n",
        "\n",
        "        # Get original Malayalam and English words\n",
        "        ml_word = ''.join([ml_idx_to_char[idx.item()] for idx in src[0] if idx.item() > 1])\n",
        "        en_word = ''.join([en_idx_to_char[idx.item()] for idx in trg if idx.item() > 3])  # Skip special tokens\n",
        "\n",
        "        # Predict with greedy decoding\n",
        "        greedy_pred = predict(model, src, src_len, en_idx_to_char, device, beam_size=1)[0]\n",
        "\n",
        "        # Predict with beam search if beam size > 1\n",
        "        beam_pred = predict(model, src, src_len, en_idx_to_char, device, beam_size=args.beam_size)[0] if args.beam_size > 1 else greedy_pred\n",
        "\n",
        "        print(f\"Example {i+1}:\")\n",
        "        print(f\"Malayalam: {ml_word}\")\n",
        "        print(f\"Target English: {en_word}\")\n",
        "        print(f\"Prediction (Greedy): {greedy_pred}\")\n",
        "        if args.beam_size > 1:\n",
        "            print(f\"Prediction (Beam Search, size={args.beam_size}): {beam_pred}\")\n",
        "        print()\n",
        "\n",
        "    # Calculate accuracy on test set\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        src, trg, src_lengths, _ = batch\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = predict(model, src, src_lengths, en_idx_to_char, device, beam_size=args.beam_size)\n",
        "\n",
        "        # Get target English words\n",
        "        for i in range(len(trg)):\n",
        "            # Skip special tokens\n",
        "            target_word = ''.join([en_idx_to_char[idx.item()] for idx in trg[i] if idx.item() > 3])\n",
        "            all_targets.append(target_word)\n",
        "\n",
        "        all_predictions.extend(predictions)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = calculate_accuracy(all_predictions, all_targets)\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Save vocabularies for later use\n",
        "    import json\n",
        "    with open('ml_char_to_idx.json', 'w') as f:\n",
        "        json.dump(ml_char_to_idx, f)\n",
        "    with open('en_char_to_idx.json', 'w') as f:\n",
        "        json.dump(en_char_to_idx, f)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "QvAhWSFAjgH6",
        "outputId": "97ff23fe-75e8-4e24-bdb6-9ddc08dc3c5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, malayalam_words, english_words, ml_char_to_idx, en_char_to_idx):\n",
        "        self.malayalam_words = malayalam_words\n",
        "        self.english_words = english_words\n",
        "        self.ml_char_to_idx = ml_char_to_idx\n",
        "        self.en_char_to_idx = en_char_to_idx\n",
        "\n",
        "        # Always use these specific tokens regardless of what's in the vocabulary\n",
        "        self.sos_index = 2  # Always use index 2 for SOS token\n",
        "        self.eos_index = 3  # Always use index 3 for EOS token\n",
        "\n",
        "        print(f\"Dataset initialized with SOS index: {self.sos_index}, EOS index: {self.eos_index}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.malayalam_words)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ml_word = self.malayalam_words[idx]\n",
        "        en_word = self.english_words[idx]\n",
        "\n",
        "        # Convert characters to indices\n",
        "        ml_indices = [self.ml_char_to_idx.get(char, self.ml_char_to_idx['<UNK>']) for char in ml_word]\n",
        "\n",
        "        # Use direct indices instead of token lookup\n",
        "        en_indices = [self.sos_index] + [self.en_char_to_idx.get(char, self.en_char_to_idx['<UNK>']) for char in en_word] + [self.eos_index]\n",
        "\n",
        "        return torch.tensor(ml_indices), torch.tensor(en_indices), len(ml_indices), len(en_indices)\n",
        "def create_vocabularies(malayalam_words, english_words):\n",
        "    # Ensure all words are strings\n",
        "    malayalam_words = [str(word) if not isinstance(word, str) else word for word in malayalam_words]\n",
        "    english_words = [str(word) if not isinstance(word, str) else word for word in english_words]\n",
        "\n",
        "    # Create Malayalam character vocabulary\n",
        "    ml_chars = set()\n",
        "    for word in malayalam_words:\n",
        "        ml_chars.update(word)\n",
        "\n",
        "    # Create English character vocabulary\n",
        "    en_chars = set()\n",
        "    for word in english_words:\n",
        "        en_chars.update(word)\n",
        "\n",
        "    # Add special tokens\n",
        "    ml_char_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    en_char_to_idx = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
        "\n",
        "    # Add characters to dictionaries\n",
        "    for i, char in enumerate(sorted(ml_chars)):\n",
        "        ml_char_to_idx[char] = i + 2  # +2 for <PAD> and <UNK>\n",
        "\n",
        "    for i, char in enumerate(sorted(en_chars)):\n",
        "        en_char_to_idx[char] = i + 4  # +4 for <PAD>, <UNK>, <SOS>, <EOS>\n",
        "\n",
        "    ml_idx_to_char = {idx: char for char, idx in ml_char_to_idx.items()}\n",
        "    en_idx_to_char = {idx: char for char, idx in en_char_to_idx.items()}\n",
        "\n",
        "    return ml_char_to_idx, en_char_to_idx, ml_idx_to_char, en_idx_to_char\n",
        "\n",
        "def collate_fn(batch):\n",
        "    ml_words, en_words, ml_lengths, en_lengths = zip(*batch)\n",
        "\n",
        "    # Pad sequences\n",
        "    ml_words_padded = pad_sequence(ml_words, batch_first=True, padding_value=0)\n",
        "    en_words_padded = pad_sequence(en_words, batch_first=True, padding_value=0)\n",
        "\n",
        "    return ml_words_padded, en_words_padded, torch.tensor(ml_lengths), torch.tensor(en_lengths)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_dim, hidden_size, num_layers, dropout, cell_type='LSTM'):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        if cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        elif cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        elif cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Cell type must be 'LSTM', 'GRU', or 'RNN'\")\n",
        "\n",
        "    def forward(self, src, src_lengths):\n",
        "        # src: [batch_size, src_len]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded: [batch_size, src_len, embedding_dim]\n",
        "\n",
        "        # Pack padded sequences for more efficient computation\n",
        "        packed_embedded = pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        if self.cell_type == 'LSTM':\n",
        "            packed_outputs, (hidden, cell) = self.rnn(packed_embedded)\n",
        "            outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "            return outputs, (hidden, cell)\n",
        "        else:\n",
        "            packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "            outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "            return outputs, hidden\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: [batch_size, hidden_size]\n",
        "        # encoder_outputs: [batch_size, src_len, hidden_size]\n",
        "\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "\n",
        "        # Repeat decoder hidden state across sequence length\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        # energy: [batch_size, src_len, hidden_size]\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "\n",
        "        # v: [hidden_size] -> [batch_size, src_len]\n",
        "        attention = torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "        # Return attention weights and apply them to encoder outputs\n",
        "        return torch.softmax(attention, dim=1)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, embedding_dim, hidden_size, num_layers, dropout, cell_type='LSTM', attention=False):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # If using attention, we'll concatenate the attention output with embedding\n",
        "        input_size = embedding_dim\n",
        "        if attention:\n",
        "            self.attention_layer = Attention(hidden_size)\n",
        "            self.fc_combine = nn.Linear(hidden_size + embedding_dim, embedding_dim)\n",
        "\n",
        "        if cell_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        elif cell_type == 'GRU':\n",
        "            self.rnn = nn.GRU(input_size, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        elif cell_type == 'RNN':\n",
        "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Cell type must be 'LSTM', 'GRU', or 'RNN'\")\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs=None):\n",
        "        # input: [batch_size, 1]\n",
        "        # hidden: tuple of [num_layers, batch_size, hidden_size] (for LSTM)\n",
        "        # encoder_outputs: [batch_size, src_len, hidden_size] (needed for attention)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded: [batch_size, 1, embedding_dim]\n",
        "\n",
        "        if self.attention and encoder_outputs is not None:\n",
        "            # Get the last hidden state from the top layer for attention\n",
        "            if self.cell_type == 'LSTM':\n",
        "                attn_hidden = hidden[0][-1]\n",
        "            else:\n",
        "                attn_hidden = hidden[-1]\n",
        "\n",
        "            # Apply attention\n",
        "            attn_weights = self.attention_layer(attn_hidden, encoder_outputs)\n",
        "            # attn_weights: [batch_size, src_len]\n",
        "\n",
        "            # Apply attention weights to encoder outputs\n",
        "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
        "            # context: [batch_size, 1, hidden_size]\n",
        "\n",
        "            # Combine context and embedded\n",
        "            embedded = self.fc_combine(torch.cat((embedded.squeeze(1), context.squeeze(1)), dim=1)).unsqueeze(1)\n",
        "\n",
        "        # Pass through RNN\n",
        "        if self.cell_type == 'LSTM':\n",
        "            output, (hidden, cell) = self.rnn(embedded, hidden)\n",
        "            output = self.fc_out(output)\n",
        "            return output, (hidden, cell)\n",
        "        else:\n",
        "            output, hidden = self.rnn(embedded, hidden)\n",
        "            output = self.fc_out(output)\n",
        "            return output, hidden\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, src_lengths, trg, teacher_forcing_ratio=0.5):\n",
        "        # src: [batch_size, src_len]\n",
        "        # trg: [batch_size, trg_len]\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_size\n",
        "\n",
        "        # Tensor to store decoder outputs\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # Encode the source sequence\n",
        "        encoder_outputs, hidden = self.encoder(src, src_lengths)\n",
        "\n",
        "        # First input to the decoder is the <SOS> token\n",
        "        input = trg[:, 0:1]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            # Use teacher forcing (use actual target as input) or not\n",
        "            use_teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            # Forward pass through decoder\n",
        "            if self.decoder.attention:\n",
        "                output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            else:\n",
        "                output, hidden = self.decoder(input, hidden)\n",
        "\n",
        "            # output: [batch_size, 1, output_size]\n",
        "            outputs[:, t:t+1, :] = output\n",
        "\n",
        "            # Get the next input for decoder\n",
        "            top1 = output.argmax(2) if not use_teacher_force else trg[:, t:t+1]\n",
        "            input = top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def infer(self, src, src_lengths, max_len=50, beam_size=1):\n",
        "        # For inference with or without beam search\n",
        "        batch_size = src.shape[0]\n",
        "\n",
        "        # Encode the source sequence\n",
        "        encoder_outputs, hidden = self.encoder(src, src_lengths)\n",
        "\n",
        "        if beam_size == 1:\n",
        "            # Simple greedy decoding\n",
        "            return self._greedy_decode(encoder_outputs, hidden, max_len)\n",
        "        else:\n",
        "            # Beam search decoding\n",
        "            return self._beam_search_decode(encoder_outputs, hidden, max_len, beam_size)\n",
        "\n",
        "    def _greedy_decode(self, encoder_outputs, hidden, max_len):\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "\n",
        "        # Start with <SOS> tokens\n",
        "        input = torch.tensor([[1]] * batch_size).to(self.device)  # Assuming 1 is <SOS>\n",
        "\n",
        "        # Lists to store predicted indices\n",
        "        predictions = torch.zeros(batch_size, max_len, dtype=torch.long).to(self.device)\n",
        "\n",
        "        for t in range(max_len):\n",
        "            # Forward pass through decoder\n",
        "            if self.decoder.attention:\n",
        "                output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            else:\n",
        "                output, hidden = self.decoder(input, hidden)\n",
        "\n",
        "            # Get most likely next token\n",
        "            top1 = output.argmax(2)\n",
        "            predictions[:, t] = top1.squeeze()\n",
        "\n",
        "            # Break if all sequences have generated <EOS>\n",
        "            if (top1 == 2).all():  # Assuming 2 is <EOS>\n",
        "                break\n",
        "\n",
        "            # Update input for next timestep\n",
        "            input = top1\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def _beam_search_decode(self, encoder_outputs, hidden, max_len, beam_size):\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "\n",
        "        # List to store final predictions for each item in batch\n",
        "        batch_predictions = []\n",
        "\n",
        "        # Process each item in batch separately for beam search\n",
        "        for b in range(batch_size):\n",
        "            # Get encoder outputs and hidden state for this item\n",
        "            single_encoder_output = encoder_outputs[b:b+1]\n",
        "\n",
        "            if self.decoder.cell_type == 'LSTM':\n",
        "                single_hidden = (hidden[0][:, b:b+1, :], hidden[1][:, b:b+1, :])\n",
        "            else:\n",
        "                single_hidden = hidden[:, b:b+1, :]\n",
        "\n",
        "            # Start with <SOS> token\n",
        "            input = torch.tensor([[1]]).to(self.device)  # Assuming 1 is <SOS>\n",
        "\n",
        "            # Lists to keep track of beams: (sequence, score, hidden_state)\n",
        "            beams = [(torch.tensor([[1]], device=self.device), 0, single_hidden)]\n",
        "            complete_beams = []\n",
        "\n",
        "            for t in range(max_len):\n",
        "                new_beams = []\n",
        "\n",
        "                # Explore each current beam\n",
        "                for sequence, score, beam_hidden in beams:\n",
        "                    # Skip completed sequences\n",
        "                    if sequence[0, -1].item() == 2:  # <EOS> token\n",
        "                        complete_beams.append((sequence, score, beam_hidden))\n",
        "                        continue\n",
        "\n",
        "                    # Use last token as input\n",
        "                    beam_input = sequence[:, -1:].to(self.device)\n",
        "\n",
        "                    # Forward pass through decoder\n",
        "                    if self.decoder.attention:\n",
        "                        output, new_hidden = self.decoder(beam_input, beam_hidden, single_encoder_output)\n",
        "                    else:\n",
        "                        output, new_hidden = self.decoder(beam_input, beam_hidden)\n",
        "\n",
        "                    # Get log probabilities\n",
        "                    log_probs = nn.functional.log_softmax(output.squeeze(1), dim=1)\n",
        "\n",
        "                    # Get top beam_size probabilities\n",
        "                    topk_probs, topk_idx = log_probs.topk(beam_size)\n",
        "\n",
        "                    # Create new beams\n",
        "                    for i in range(beam_size):\n",
        "                        token = topk_idx[0, i].unsqueeze(0).unsqueeze(0)\n",
        "                        new_seq = torch.cat([sequence, token], dim=1)\n",
        "                        new_score = score + topk_probs[0, i].item()\n",
        "                        new_beams.append((new_seq, new_score, new_hidden))\n",
        "\n",
        "                # Keep only the top beam_size beams\n",
        "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
        "\n",
        "                # Stop if all beams end with <EOS>\n",
        "                if all(beam[0][0, -1].item() == 2 for beam in beams):\n",
        "                    complete_beams.extend(beams)\n",
        "                    break\n",
        "\n",
        "            # Handle case if no complete beams (no <EOS> found)\n",
        "            if not complete_beams:\n",
        "                complete_beams = beams\n",
        "\n",
        "            # Sort completed beams by score and return highest scoring sequence\n",
        "            best_beam = max(complete_beams, key=lambda x: x[1])\n",
        "            batch_predictions.append(best_beam[0])\n",
        "\n",
        "        # Pad sequences to same length for batch\n",
        "        max_pred_len = max(pred.shape[1] for pred in batch_predictions)\n",
        "        padded_preds = torch.zeros(batch_size, max_pred_len, dtype=torch.long).to(self.device)\n",
        "\n",
        "        for i, pred in enumerate(batch_predictions):\n",
        "            padded_preds[i, :pred.shape[1]] = pred\n",
        "\n",
        "        return padded_preds\n",
        "\n",
        "\n",
        "def create_vocabularies(malayalam_words, english_words):\n",
        "    # Ensure all words are strings\n",
        "    malayalam_words = [str(word) if not isinstance(word, str) else word for word in malayalam_words]\n",
        "    english_words = [str(word) if not isinstance(word, str) else word for word in english_words]\n",
        "\n",
        "    # Create Malayalam character vocabulary\n",
        "    ml_chars = set()\n",
        "    for word in malayalam_words:\n",
        "        ml_chars.update(word)\n",
        "\n",
        "    # Create English character vocabulary\n",
        "    en_chars = set()\n",
        "    for word in english_words:\n",
        "        en_chars.update(word)\n",
        "\n",
        "    # Add special tokens\n",
        "    ml_char_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    en_char_to_idx = {'<PAD>': 0, '<UNK>': 1, '< SOS >': 2, '<EOS>': 3}\n",
        "\n",
        "    # Add characters to dictionaries\n",
        "    for i, char in enumerate(sorted(ml_chars)):\n",
        "        ml_char_to_idx[char] = i + 2  # +2 for <PAD> and <UNK>\n",
        "\n",
        "    for i, char in enumerate(sorted(en_chars)):\n",
        "        en_char_to_idx[char] = i + 4  # +4 for <PAD>, <UNK>, < SOS >, <EOS>\n",
        "\n",
        "    ml_idx_to_char = {idx: char for char, idx in ml_char_to_idx.items()}\n",
        "    en_idx_to_char = {idx: char for char, idx in en_char_to_idx.items()}\n",
        "\n",
        "    return ml_char_to_idx, en_char_to_idx, ml_idx_to_char, en_idx_to_char\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            src, trg, src_lengths, trg_lengths = batch\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            output = model(src, src_lengths, trg, teacher_forcing_ratio=0)\n",
        "\n",
        "            # Reshape for computing loss\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(output, trg)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, optimizer, criterion, n_epochs, device, clip=1, teacher_forcing_ratio=0.5):\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{n_epochs}'):\n",
        "            src, trg, src_lengths, trg_lengths = batch\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(src, src_lengths, trg, teacher_forcing_ratio)\n",
        "\n",
        "            # Reshape for computing loss\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_loss = evaluate(model, val_dataloader, criterion, device)\n",
        "\n",
        "        # Save losses for plotting\n",
        "        train_losses.append(epoch_loss / len(train_dataloader))\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02}')\n",
        "        print(f'\\tTrain Loss: {epoch_loss / len(train_dataloader):.4f}')\n",
        "        print(f'\\tValidation Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_transliteration_model.pt')\n",
        "            print(f'\\tBest model saved with validation loss: {val_loss:.4f}')\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "def predict(model, src, src_lengths, en_idx_to_char, device, beam_size=1):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Forward pass with beam search or greedy decoding\n",
        "        predictions = model.infer(src.to(device), src_lengths, beam_size=beam_size)\n",
        "\n",
        "    # Convert predictions to characters\n",
        "    predicted_words = []\n",
        "    for pred in predictions:\n",
        "        word = []\n",
        "        for idx in pred:\n",
        "            idx = idx.item()\n",
        "            if idx == 3:  # <EOS> token\n",
        "                break\n",
        "            if idx > 3:  # Skip special tokens\n",
        "                word.append(en_idx_to_char[idx])\n",
        "        predicted_words.append(''.join(word))\n",
        "\n",
        "    return predicted_words\n",
        "\n",
        "\n",
        "def calculate_accuracy(predicted_words, target_words):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for pred, target in zip(predicted_words, target_words):\n",
        "        target = str(target)  # Convert target to string in case it's not\n",
        "        if pred == target:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    return (correct / total) * 100.0 if total > 0 else 0.0\n",
        "\n",
        "\n",
        "def plot_learning_curves(train_losses, val_losses):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('learning_curves.png')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def main(args=None):\n",
        "    # If running in a Jupyter/Colab environment, allow parameters to be passed directly\n",
        "    # Otherwise, use command line arguments\n",
        "    if args is None:\n",
        "        parser = argparse.ArgumentParser(description='Malayalam to English Transliteration')\n",
        "        parser.add_argument('--embedding_dim', type=int, default=64, choices=[16, 32, 64, 256],\n",
        "                            help='Embedding dimension size')\n",
        "        parser.add_argument('--encoder_layers', type=int, default=2, choices=[1, 2, 3],\n",
        "                            help='Number of encoder layers')\n",
        "        parser.add_argument('--decoder_layers', type=int, default=2, choices=[1, 2, 3],\n",
        "                            help='Number of decoder layers')\n",
        "        parser.add_argument('--hidden_size', type=int, default=64, choices=[16, 32, 64, 256],\n",
        "                            help='Hidden layer size')\n",
        "        parser.add_argument('--cell_type', type=str, default='LSTM', choices=['RNN', 'GRU', 'LSTM'],\n",
        "                            help='RNN cell type')\n",
        "        parser.add_argument('--dropout', type=float, default=0.2, choices=[0.2, 0.3],\n",
        "                            help='Dropout rate')\n",
        "        parser.add_argument('--attention', action='store_true',\n",
        "                            help='Use attention mechanism')\n",
        "        parser.add_argument('--teacher_forcing', type=float, default=0.5,\n",
        "                            help='Teacher forcing ratio (0 to disable)')\n",
        "        parser.add_argument('--beam_size', type=int, default=1,\n",
        "                            help='Beam size for beam search decoding (1 for greedy)')\n",
        "        parser.add_argument('--batch_size', type=int, default=64,\n",
        "                            help='Batch size for training')\n",
        "        parser.add_argument('--epochs', type=int, default=15,\n",
        "                            help='Number of training epochs')\n",
        "        parser.add_argument('--learning_rate', type=float, default=0.001,\n",
        "                            help='Learning rate')\n",
        "        parser.add_argument('--data_path', type=str, default='/content/ml/lexicons/ml.translit.sampled.train.tsv',\n",
        "                            help='Path to training data')\n",
        "        parser.add_argument('--val_path', type=str, default='/content/ml/lexicons/ml.translit.sampled.dev.tsv',\n",
        "                            help='Path to validation data')\n",
        "        parser.add_argument('--test_path', type=str, default='/content/ml/lexicons/ml.translit.sampled.test.tsv',\n",
        "                            help='Path to test data')\n",
        "\n",
        "        try:\n",
        "            args = parser.parse_args()\n",
        "        except SystemExit:\n",
        "            # If running in Jupyter/Colab, use default arguments\n",
        "            print(\"Using default arguments since we appear to be in a notebook environment\")\n",
        "            class Args:\n",
        "                def __init__(self):\n",
        "                    self.embedding_dim = 64\n",
        "                    self.encoder_layers = 2\n",
        "                    self.decoder_layers = 2\n",
        "                    self.hidden_size = 64\n",
        "                    self.cell_type = 'LSTM'\n",
        "                    self.dropout = 0.2\n",
        "                    self.attention = True\n",
        "                    self.teacher_forcing = 0.5\n",
        "                    self.beam_size = 1\n",
        "                    self.batch_size = 64\n",
        "                    self.epochs = 15\n",
        "                    self.learning_rate = 0.001\n",
        "                    self.data_path = '/content/ml/lexicons/ml.translit.sampled.train.tsv'\n",
        "                    self.val_path = '/content/ml/lexicons/ml.translit.sampled.dev.tsv'\n",
        "                    self.test_path = '/content/ml/lexicons/ml.translit.sampled.test.tsv'\n",
        "            args = Args()\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    try:\n",
        "        df = pd.read_csv(args.data_path, sep='\\t', header=None).dropna()\n",
        "        df_val = pd.read_csv(args.val_path, sep='\\t', header=None).dropna()\n",
        "        df_test = pd.read_csv(args.test_path, sep='\\t', header=None).dropna()\n",
        "\n",
        "        print(f\"Data loaded successfully:\")\n",
        "        print(f\"Training samples: {len(df)}\")\n",
        "        print(f\"Validation samples: {len(df_val)}\")\n",
        "        print(f\"Test samples: {len(df_test)}\")\n",
        "\n",
        "        # Check data types\n",
        "        print(\"\\nChecking data types:\")\n",
        "        print(f\"Training data types: {df.dtypes}\")\n",
        "\n",
        "        # Display a few samples\n",
        "        print(\"\\nSample data:\")\n",
        "        print(df.head(3))\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: File not found. {e}\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return\n",
        "\n",
        "    # Split into native (Malayalam) and romanized (English) columns\n",
        "    malayalam_words = df[0].tolist()\n",
        "    english_words = df[1].tolist()\n",
        "    print(f\"Number of Malayalam words: {len(malayalam_words)}\")\n",
        "    print(f\"Number of English words: {len(english_words)}\")\n",
        "\n",
        "    malayalam_words_val = df_val[0].tolist()\n",
        "    english_words_val = df_val[1].tolist()\n",
        "\n",
        "    malayalam_words_test = df_test[0].tolist()\n",
        "    english_words_test = df_test[1].tolist()\n",
        "\n",
        "    # Check for non-string values and convert if needed\n",
        "    print(\"\\nConverting data types if needed...\")\n",
        "\n",
        "    # Create vocabularies\n",
        "    ml_char_to_idx, en_char_to_idx, ml_idx_to_char, en_idx_to_char = create_vocabularies(\n",
        "        malayalam_words + malayalam_words_val + malayalam_words_test,\n",
        "        english_words + english_words_val + english_words_test\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TransliterationDataset(malayalam_words, english_words, ml_char_to_idx, en_char_to_idx)\n",
        "    val_dataset = TransliterationDataset(malayalam_words_val, english_words_val, ml_char_to_idx, en_char_to_idx)\n",
        "    test_dataset = TransliterationDataset(malayalam_words_test, english_words_test, ml_char_to_idx, en_char_to_idx)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Create model\n",
        "    input_size = len(ml_char_to_idx)\n",
        "    output_size = len(en_char_to_idx)\n",
        "\n",
        "    encoder = Encoder(\n",
        "        input_size=input_size,\n",
        "        embedding_dim=args.embedding_dim,\n",
        "        hidden_size=args.hidden_size,\n",
        "        num_layers=args.encoder_layers,\n",
        "        dropout=args.dropout,\n",
        "        cell_type=args.cell_type\n",
        "    )\n",
        "\n",
        "    decoder = Decoder(\n",
        "        output_size=output_size,\n",
        "        embedding_dim=args.embedding_dim,\n",
        "        hidden_size=args.hidden_size,\n",
        "        num_layers=args.decoder_layers,\n",
        "        dropout=args.dropout,\n",
        "        cell_type=args.cell_type,\n",
        "        attention=args.attention\n",
        "    )\n",
        "\n",
        "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
        "\n",
        "    # Print model architecture and hyperparameters\n",
        "    print(f\"Model Architecture:\")\n",
        "    print(f\"Input Size: {input_size}\")\n",
        "    print(f\"Output Size: {output_size}\")\n",
        "    print(f\"Embedding Dimension: {args.embedding_dim}\")\n",
        "    print(f\"Hidden Size: {args.hidden_size}\")\n",
        "    print(f\"Encoder Layers: {args.encoder_layers}\")\n",
        "    print(f\"Decoder Layers: {args.decoder_layers}\")\n",
        "    print(f\"Cell Type: {args.cell_type}\")\n",
        "    print(f\"Dropout: {args.dropout}\")\n",
        "    print(f\"Attention: {args.attention}\")\n",
        "    print(f\"Teacher Forcing Ratio: {args.teacher_forcing}\")\n",
        "    print(f\"Beam Size: {args.beam_size}\")\n",
        "    print(f\"Batch Size: {args.batch_size}\")\n",
        "    print(f\"Learning Rate: {args.learning_rate}\")\n",
        "    print(f\"Number of Epochs: {args.epochs}\")\n",
        "\n",
        "    # Train model\n",
        "    start_time = time.time()\n",
        "    train_losses, val_losses = train(\n",
        "        model,\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        args.epochs,\n",
        "        device,\n",
        "        teacher_forcing_ratio=args.teacher_forcing\n",
        "    )\n",
        "    print(f\"Training completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # Plot learning curves\n",
        "    plot_learning_curves(train_losses, val_losses)\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load('best_transliteration_model.pt'))\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss = evaluate(model, test_dataloader, criterion, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Test some examples with and without beam search\n",
        "    num_examples = min(5, len(test_dataset))\n",
        "    for i in range(num_examples):\n",
        "        src, trg, src_len, trg_len = test_dataset[i]\n",
        "        src = src.unsqueeze(0)  # Add batch dimension\n",
        "        src_len = torch.tensor([src_len])\n",
        "\n",
        "        # Get original Malayalam and English words\n",
        "        ml_word = ''.join([ml_idx_to_char[idx.item()] for idx in src[0] if idx.item() > 1])\n",
        "        en_word = ''.join([en_idx_to_char[idx.item()] for idx in trg if idx.item() > 3])  # Skip special tokens\n",
        "\n",
        "        # Predict with greedy decoding\n",
        "        greedy_pred = predict(model, src, src_len, en_idx_to_char, device, beam_size=1)[0]\n",
        "\n",
        "        # Predict with beam search if beam size > 1\n",
        "        beam_pred = predict(model, src, src_len, en_idx_to_char, device, beam_size=args.beam_size)[0] if args.beam_size > 1 else greedy_pred\n",
        "\n",
        "        print(f\"Example {i+1}:\")\n",
        "        print(f\"Malayalam: {ml_word}\")\n",
        "        print(f\"Target English: {en_word}\")\n",
        "        print(f\"Prediction (Greedy): {greedy_pred}\")\n",
        "        if args.beam_size > 1:\n",
        "            print(f\"Prediction (Beam Search, size={args.beam_size}): {beam_pred}\")\n",
        "        print()\n",
        "\n",
        "    # Calculate accuracy on test set\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        src, trg, src_lengths, _ = batch\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = predict(model, src, src_lengths, en_idx_to_char, device, beam_size=args.beam_size)\n",
        "\n",
        "        # Get target English words\n",
        "        for i in range(len(trg)):\n",
        "            # Skip special tokens\n",
        "            target_word = ''.join([en_idx_to_char[idx.item()] for idx in trg[i] if idx.item() > 3])\n",
        "            all_targets.append(target_word)\n",
        "\n",
        "        all_predictions.extend(predictions)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = calculate_accuracy(all_predictions, all_targets)\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Save vocabularies for later use\n",
        "    import json\n",
        "    with open('ml_char_to_idx.json', 'w') as f:\n",
        "        json.dump(ml_char_to_idx, f)\n",
        "    with open('en_char_to_idx.json', 'w') as f:\n",
        "        json.dump(en_char_to_idx, f)\n",
        "\n",
        "\n",
        "# Function to run the model with specified parameters\n",
        "def run_transliteration_model(\n",
        "    embedding_dim=64,\n",
        "    encoder_layers=2,\n",
        "    decoder_layers=2,\n",
        "    hidden_size=64,\n",
        "    cell_type='LSTM',\n",
        "    dropout=0.2,\n",
        "    attention=True,\n",
        "    teacher_forcing=0.5,\n",
        "    beam_size=1,\n",
        "    batch_size=64,\n",
        "    epochs=15,\n",
        "    learning_rate=0.001,\n",
        "    data_path='/content/ml/lexicons/ml.translit.sampled.train.tsv',\n",
        "    val_path='/content/ml/lexicons/ml.translit.sampled.dev.tsv',\n",
        "    test_path='/content/ml/lexicons/ml.translit.sampled.test.tsv'\n",
        "):\n",
        "    \"\"\"\n",
        "    Run the Malayalam to English transliteration model with specified parameters.\n",
        "    This function can be called directly from a Jupyter/Colab notebook.\n",
        "    \"\"\"\n",
        "    class Args:\n",
        "        pass\n",
        "\n",
        "    args = Args()\n",
        "    args.embedding_dim = embedding_dim\n",
        "    args.encoder_layers = encoder_layers\n",
        "    args.decoder_layers = decoder_layers\n",
        "    args.hidden_size = hidden_size\n",
        "    args.cell_type = cell_type\n",
        "    args.dropout = dropout\n",
        "    args.attention = attention\n",
        "    args.teacher_forcing = teacher_forcing\n",
        "    args.beam_size = beam_size\n",
        "    args.batch_size = batch_size\n",
        "    args.epochs = epochs\n",
        "    args.learning_rate = learning_rate\n",
        "    args.data_path = data_path\n",
        "    args.val_path = val_path\n",
        "    args.test_path = test_path\n",
        "\n",
        "    main(args)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "PmCAKftdjgGc",
        "outputId": "72430e9f-99e3-49c4-bf90-36bb29ce1adb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--embedding_dim {16,32,64,256}]\n",
            "                                [--encoder_layers {1,2,3}]\n",
            "                                [--decoder_layers {1,2,3}]\n",
            "                                [--hidden_size {16,32,64,256}]\n",
            "                                [--cell_type {RNN,GRU,LSTM}]\n",
            "                                [--dropout {0.2,0.3}] [--attention]\n",
            "                                [--teacher_forcing TEACHER_FORCING]\n",
            "                                [--beam_size BEAM_SIZE]\n",
            "                                [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
            "                                [--learning_rate LEARNING_RATE]\n",
            "                                [--data_path DATA_PATH] [--val_path VAL_PATH]\n",
            "                                [--test_path TEST_PATH]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-397ee6ca-b4ca-47be-a75f-46cf1938a8bb.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using default arguments since we appear to be in a notebook environment\n",
            "Using device: cuda\n",
            "Data loaded successfully:\n",
            "Training samples: 58381\n",
            "Validation samples: 5641\n",
            "Test samples: 5610\n",
            "\n",
            "Checking data types:\n",
            "Training data types: 0    object\n",
            "1    object\n",
            "2     int64\n",
            "dtype: object\n",
            "\n",
            "Sample data:\n",
            "     0     1  2\n",
            "0   അം    am  3\n",
            "1  അംഗ  amga  2\n",
            "2  അംഗ  anga  1\n",
            "Number of Malayalam words: 58381\n",
            "Number of English words: 58381\n",
            "\n",
            "Converting data types if needed...\n",
            "Dataset initialized with SOS index: 2, EOS index: 3\n",
            "Dataset initialized with SOS index: 2, EOS index: 3\n",
            "Dataset initialized with SOS index: 2, EOS index: 3\n",
            "Model Architecture:\n",
            "Input Size: 72\n",
            "Output Size: 30\n",
            "Embedding Dimension: 64\n",
            "Hidden Size: 64\n",
            "Encoder Layers: 2\n",
            "Decoder Layers: 2\n",
            "Cell Type: LSTM\n",
            "Dropout: 0.2\n",
            "Attention: True\n",
            "Teacher Forcing Ratio: 0.5\n",
            "Beam Size: 1\n",
            "Batch Size: 64\n",
            "Learning Rate: 0.001\n",
            "Number of Epochs: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/15:  51%|█████     | 464/913 [00:33<00:30, 14.79it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MeA-7-iajgEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QcrxDlS9jgDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VdcbC-2BjgBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ReWzuzAljf65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
        "  \"parameters\": {\n",
        "    \"embedding_size\": {\"values\": [32, 64, 128]},\n",
        "    \"hidden_size\": {\"values\": [64, 128]},\n",
        "    \"num_encoder_layers\": {\"values\": [1, 2]},\n",
        "    \"num_decoder_layers\": {\"values\": [1, 2]},\n",
        "    \"cell_type\": {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n",
        "    \"dropout\": {\"values\": [0.2, 0.3]},\n",
        "    \"beam_size\": {\"values\": [1, 3, 5]},\n",
        "    \"learning_rate\": {\"values\": [1e-3, 5e-4]}\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "id": "j80CD7Fue-vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"DL-Assignment3\")\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"DL-Assignment3\")\n",
        "wandb.agent(sweep_id, function=train_function)\n"
      ],
      "metadata": {
        "id": "XKc6fQRxe-tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6BbY3Igte-qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,input_size, embedding_size, hidden_size, enc_layers, p, cell_type, bidirectional):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.enc_layers = enc_layers\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.cell_type = cell_type\n",
        "    self.bidirectional = bidirectional\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    if(cell_type == \"GRU\"):\n",
        "      self.gru = nn.GRU(embedding_size, hidden_size, enc_layers, dropout = p, bidirectional = bidirectional)\n",
        "    if(cell_type == \"RNN\"):\n",
        "      self.rnn = nn.RNN(embedding_size, hidden_size, enc_layers, dropout = p, bidirectional = bidirectional)\n",
        "    if(cell_type == \"LSTM\"):\n",
        "      self.lstm = nn.LSTM(embedding_size, hidden_size, enc_layers, dropout = p, bidirectional = bidirectional)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    # embedding shape : (seq_length, N, embedding_size)\n",
        "    if(self.cell_type == \"GRU\"):\n",
        "      output, hidden = self.gru(embedding)\n",
        "    if(self.cell_type == \"RNN\"):\n",
        "      output, hidden = self.rnn(embedding)\n",
        "    if(self.cell_type == \"LSTM\"):\n",
        "      outputs, (hidden,cell) = self.lstm(embedding)\n",
        "      return outputs, hidden, cell\n",
        "    return output, hidden\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, output_size, dec_layers, p, cell_type):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.dec_layers = dec_layers\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.cell_type = cell_type\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    if(cell_type == \"GRU\"):\n",
        "      self.gru = nn.GRU(embedding_size, hidden_size, dec_layers, dropout = p)\n",
        "    if(cell_type == \"RNN\"):\n",
        "      self.rnn = nn.RNN(embedding_size, hidden_size, dec_layers, dropout = p)\n",
        "    if(cell_type == \"LSTM\"):\n",
        "      self.lstm = nn.LSTM(embedding_size, hidden_size, dec_layers, dropout = p)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)  # fully connected.\n",
        "\n",
        "  def forward(self,x,output, hidden, cell = 0):\n",
        "    # shape of x: (N) but we want (1,N)\n",
        "    x = x.unsqueeze(0).int()\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    # embedding shape : (1,N,embedding_size)\n",
        "    if(self.cell_type == \"GRU\"):\n",
        "        outputs, hidden = self.gru(embedding, hidden)\n",
        "    if(self.cell_type == \"RNN\"):\n",
        "        outputs, hidden = self.rnn(embedding, hidden)\n",
        "    if(self.cell_type == \"LSTM\"):\n",
        "        outputs, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
        "    # shape of outputs: (1, N, hidden_size)\n",
        "    predictions = self.fc(outputs)\n",
        "    # shape of predictions: (1, N, length_of_vocab)\n",
        "    predictions = predictions.squeeze(0)\n",
        "    # shape of predictions: (N, length_of_vocab)\n",
        "    if(self.cell_type == \"LSTM\"):\n",
        "        return predictions, hidden, cell\n",
        "    return predictions, hidden\n",
        "\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "class Atten_decoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, output_size, dec_layers, p, cell_type, bidirectional):\n",
        "    super(Atten_decoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.max_length = len(english_matrix[0])  #30\n",
        "    self.dec_layers = dec_layers\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.cell_type = cell_type\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    if(cell_type == \"GRU\"):\n",
        "      self.gru = nn.GRU(hidden_size, hidden_size, dec_layers, dropout = p)\n",
        "    if(cell_type == \"RNN\"):\n",
        "      self.rnn = nn.RNN(hidden_size, hidden_size, dec_layers, dropout = p)\n",
        "    if(cell_type == \"LSTM\"):\n",
        "      self.lstm = nn.LSTM(hidden_size, hidden_size, dec_layers, dropout = p)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    self.attn = nn.Linear(hidden_size+embedding_size, self.max_length)\n",
        "    if(bidirectional):\n",
        "      self.attn_combine = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n",
        "    else :\n",
        "      self.attn_combine = nn.Linear(hidden_size + embedding_size, hidden_size)\n",
        "\n",
        "  def forward(self, x,output, hidden, cell = 0):\n",
        "    x = x.unsqueeze(0)\n",
        "    output=output.permute(1,0,2)\n",
        "    embedded = self.embedding(x)\n",
        "    embedded = self.dropout(embedded)\n",
        "    attn_weights = F.softmax(self.attn(torch.cat((embedded[0],hidden[0]), 1)), dim = 1)\n",
        "    attn_applied = torch.bmm(attn_weights.unsqueeze(1),output)\n",
        "    attn_applied = attn_applied.squeeze(1)\n",
        "    op = torch.cat((embedded[0], attn_applied), 1)\n",
        "\n",
        "    op = self.attn_combine(op).unsqueeze(0)\n",
        "    op = F.relu(op)\n",
        "    if(self.cell_type == \"GRU\"):\n",
        "        outputs, hidden = self.gru(op, hidden)\n",
        "    if(self.cell_type == \"RNN\"):\n",
        "        outputs, hidden = self.rnn(op, hidden)\n",
        "    if(self.cell_type == \"LSTM\"):\n",
        "        outputs, (hidden, cell) = self.lstm(op, (hidden, cell))\n",
        "    predictions = self.fc(outputs)\n",
        "    # shape of predictions: (1, N, length_of_vocab)\n",
        "    predictions = predictions.squeeze(0)\n",
        "    # shape of predictions: (N, length_of_vocab)\n",
        "    if(self.cell_type == \"LSTM\"):\n",
        "        return predictions, hidden, cell\n",
        "    return predictions, hidden\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, cell_type, bidirectional, enc_layers, dec_layers):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.cell_type = cell_type\n",
        "        self.bidirectional = bidirectional\n",
        "        self.enc_layers = enc_layers\n",
        "        self.dec_layers = dec_layers\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = len(malayalam_chars) + 2\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "        if(self.cell_type == \"LSTM\"):\n",
        "            encoder_output, hidden, cell = self.encoder(source)\n",
        "        else:\n",
        "            encoder_output, hidden = self.encoder(source)\n",
        "        # if(self.bidirectional == True):\n",
        "        if(self.enc_layers != self.dec_layers or self.bidirectional == True):\n",
        "          hidden = hidden[self.enc_layers - 1] + hidden[self.enc_layers - 1]\n",
        "          hidden = hidden.repeat(self.dec_layers,1,1)\n",
        "          if(self.cell_type == \"LSTM\"):\n",
        "              cell = cell[self.enc_layers - 1] + cell[self.enc_layers - 1]\n",
        "              cell = cell.repeat(self.dec_layers,1,1)\n",
        "\n",
        "        x = target[0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "#             print(\"STARTED t= \",t)\n",
        "            if(self.cell_type == \"LSTM\"):\n",
        "                output, hidden, cell = self.decoder(x, encoder_output, hidden, cell)\n",
        "            else :\n",
        "                output, hidden = self.decoder(x, encoder_output, hidden)\n",
        "            outputs[t] = output\n",
        "\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "#         print(\"decoder sucessful\")\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "OpsREp3Ne-Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip, device, teacher_force_ratio=0.5):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for src, trg in iterator:\n",
        "        src = src.to(device).transpose(0,1)  # (seq_len, batch)\n",
        "        trg = trg.to(device).transpose(0,1)  # (seq_len, batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg, teacher_force_ratio)  # with teacher forcing\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].reshape(-1, output_dim)\n",
        "        trg_y = trg[1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg_y)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        preds = output.argmax(dim=1)\n",
        "        non_pad = trg_y != criterion.ignore_index\n",
        "        correct = (preds == trg_y) & non_pad\n",
        "\n",
        "        epoch_acc += correct.sum().item()\n",
        "        total_tokens += non_pad.sum().item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / total_tokens\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion, device, beam_size=1):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in iterator:\n",
        "            src = src.to(device).transpose(0,1)\n",
        "            trg = trg.to(device).transpose(0,1)\n",
        "\n",
        "            if beam_size == 1:\n",
        "                # no beam search; greedy decoding (teacher forcing off)\n",
        "                output = model(src, trg, teacher_force_ratio=0)\n",
        "            else:\n",
        "                # TODO: Implement beam search decoding here\n",
        "                output = beam_search_decode(model, src, beam_size, device)\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output_flat = output[1:].reshape(-1, output_dim)\n",
        "            trg_flat = trg[1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output_flat, trg_flat)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            preds = output_flat.argmax(dim=1)\n",
        "            non_pad = trg_flat != criterion.ignore_index\n",
        "            correct = (preds == trg_flat) & non_pad\n",
        "\n",
        "            epoch_acc += correct.sum().item()\n",
        "            total_tokens += non_pad.sum().item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / total_tokens\n",
        "\n",
        "\n",
        "def wandb_trainer(config=None):\n",
        "    # Initialize wandb run with sweep config\n",
        "    wandb.init(config=config)\n",
        "    config = wandb.config\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Setup model based on config parameters\n",
        "    encoder_net = Encoder(\n",
        "        input_size=len(english_chars) + 2,\n",
        "        embedding_size=config.embedding_size,\n",
        "        hidden_size=config.hidden_size,\n",
        "        enc_layers=config.enc_layers,\n",
        "        p=config.dropout,\n",
        "        cell_type=config.cell_type,\n",
        "        bidirectional=config.bidirectional\n",
        "    ).to(device)\n",
        "\n",
        "    if config.attention:\n",
        "        decoder_net = Atten_decoder(\n",
        "            input_size=len(malayalam_chars) + 2,\n",
        "            embedding_size=config.embedding_size,\n",
        "            hidden_size=config.hidden_size,\n",
        "            output_size=len(malayalam_chars) + 2,\n",
        "            dec_layers=config.dec_layers,\n",
        "            p=config.dropout,\n",
        "            cell_type=config.cell_type,\n",
        "            bidirectional=config.bidirectional\n",
        "        ).to(device)\n",
        "    else:\n",
        "        decoder_net = Decoder(\n",
        "            input_size=len(malayalam_chars) + 2,\n",
        "            embedding_size=config.embedding_size,\n",
        "            hidden_size=config.hidden_size,\n",
        "            output_size=len(malayalam_chars) + 2,\n",
        "            dec_layers=config.dec_layers,\n",
        "            p=config.dropout,\n",
        "            cell_type=config.cell_type\n",
        "        ).to(device)\n",
        "\n",
        "    model = Seq2Seq(\n",
        "        encoder_net,\n",
        "        decoder_net,\n",
        "        config.cell_type,\n",
        "        config.bidirectional,\n",
        "        config.enc_layers,\n",
        "        config.dec_layers\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "    pad_idx = len(malayalam_chars) + 1\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "    CLIP = 1\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, CLIP, device, config.teacher_force_ratio)\n",
        "        valid_loss, valid_acc = evaluate(model, val_loader, criterion, device, beam_size=config.beam_size)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config.epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2%}\")\n",
        "        print(f\"Val Loss: {valid_loss:.3f} | Val Acc: {valid_acc:.2%}\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": valid_loss,\n",
        "            \"val_accuracy\": valid_acc,\n",
        "            \"epoch\": epoch\n",
        "        })\n",
        "\n",
        "    # Optional: test evaluation after training ends\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device, beam_size=config.beam_size)\n",
        "    print(f\"Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.2%}\")\n",
        "    wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_acc})\n",
        "\n"
      ],
      "metadata": {
        "id": "bUQyZQqJcWF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters / setup (example)\n",
        "clip = 1\n",
        "teacher_force_ratio = 0.5\n",
        "\n",
        "# Run one epoch of training\n",
        "train_loss, train_acc = train(\n",
        "    model=model,\n",
        "    iterator=train_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    clip=clip,\n",
        "    device=device,\n",
        "    teacher_force_ratio=teacher_force_ratio\n",
        ")\n",
        "\n",
        "print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc*100:.2f}%\")\n",
        "\n",
        "# Run one epoch of evaluation (test)\n",
        "val_loss, val_acc = evaluate(\n",
        "    model=model,\n",
        "    iterator=val_loader,\n",
        "    criterion=criterion,\n",
        "    device=device,\n",
        "    beam_size=1  # greedy decoding\n",
        ")\n",
        "\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "FIA6V8qsiOJ2",
        "outputId": "564fd2df-a28a-43bb-8cd3-3663c3f5be2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.3874, Train Accuracy: 62.73%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Seq2Seq.forward() got an unexpected keyword argument 'teacher_force_ratio'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-908aa4df766c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Run one epoch of evaluation (test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m val_loss, val_acc = evaluate(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0miterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-131-91c59a5703ca>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, iterator, criterion, device, beam_size)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbeam_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;31m# no beam search; greedy decoding (teacher forcing off)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_force_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;31m# TODO: Implement beam search decoding here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Seq2Seq.forward() got an unexpected keyword argument 'teacher_force_ratio'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Question 3 (15 Marks)\n",
        "Based on the above plots write down some insightful observations. For example,\n",
        "- RNN based model takes longer time to converge than GRU or LSTM\n",
        "- using smaller sizes for the hidden layer does not give good results\n",
        "- dropout leads to better performance\n",
        "\n",
        "(Note: I don't know if any of the above statements is true. I just wrote some random comments that came to my mind)\n",
        "\n",
        "Of course, each inference should be backed by appropriate evidence."
      ],
      "metadata": {
        "id": "4VHpsNBIa100"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F82exrs7cWwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Question 4 (10 Marks)\n",
        "\n",
        "You will now apply your best model on the test data (You shouldn't have used test data so far. All the above experiments should have been done using train and val data only).\n",
        "\n",
        "(a) Use the best model from your sweep and report the accuracy on the test set (the output is correct only if it exactly matches the reference output).\n",
        "\n",
        "(b) Provide sample inputs from the test data and predictions made by your best model (more marks for presenting this grid creatively). Also upload all the predictions on the test set in a folder **predictions_vanilla** on your github project.\n",
        "\n",
        "(c) Comment on the errors made by your model (simple insightful bullet points)\n",
        "\n",
        "- The model makes more errors on consonants than vowels\n",
        "- The model makes more errors on longer sequences\n",
        "- I am thinking confusion matrix but may be it's just me!\n",
        "- ..."
      ],
      "metadata": {
        "id": "xfTCrCc0a495"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r-wFgTO8cXU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Question 5 (20 Marks)\n",
        "\n",
        "Now add an attention network to your basis sequence to sequence model and train the model again. For the sake of simplicity you can use a single layered encoder and a single layered decoder (if you want you can use multiple layers also). Please answer the following questions:\n",
        "\n",
        "(a) Did you tune the hyperparameters again? If yes please paste appropriate plots below.\n",
        "\n",
        "(b) Evaluate your best model on the test set and report the accuracy. Also upload all the predictions on the test set in a folder **predictions_attention** on your github project.\n",
        "\n",
        "(c) Does the attention based model perform better than the vanilla model? If so, can you check some of the errors that this model corrected and note down your inferences (i.e., outputs which were predicted incorrectly by your best seq2seq model are predicted correctly by this model)\n",
        "\n",
        "(d) In a 3 x 3 grid paste the attention heatmaps for 10 inputs from your test data (read up on what are attention heatmaps)."
      ],
      "metadata": {
        "id": "0YRbeDsHa8jp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4ZGQkVWcYZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Question 6 (20 Marks)\n",
        "\n",
        "This a challenge question and most of you will find it hard.\n",
        "\n",
        "I like the visualisation in the figure captioned \"Connectivity\" in this [article](https://distill.pub/2019/memorization-in-rnns/#appendix-autocomplete). Make a similar visualisation for your model. Please look at this [blog](https://towardsdatascience.com/visualising-lstm-activations-in-keras-b50206da96ff) for some starter code. The goal is to figure out the following: When the model is decoding the $i$-th character in the output which is the input character that it is looking at?\n",
        "\n",
        "Have fun!\n"
      ],
      "metadata": {
        "id": "6pUsxVo6a_P2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xmaZ1KnWcY1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Question 7 (10 Marks)\n",
        "Paste a link to your github code for Part A\n",
        "\n",
        "Example: https://github.com/&lt;user-id&gt;/da6401_assignment3/partA;\n",
        "\n",
        "- We will check for coding style, clarity in using functions and a README file with clear instructions on training and evaluating the model (the 10 marks will be based on this).\n",
        "\n",
        "- We will also run a plagiarism check to ensure that the code is not copied (0 marks in the assignment if we find that the code is plagiarised).\n",
        "\n",
        "- We will check the number of commits made by the two team members and then give marks accordingly. For example, if we see 70% of the commits were made by one team member then that member will get more marks in the assignment (**note that this contribution will decide the marks split for the entire assignment and not just this question**).\n",
        "\n",
        "- We will also check if the training and test splits have been used properly. You will get 0 marks on the assignment if we find any cheating (e.g., adding test data to training data) to get higher accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "gSnrEA_0bDJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EGLYAZyJcZcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Question 8 (0 Marks)\n",
        "\n",
        "Note that this question does not carry any marks and will not be graded. This is only for students who are looking for a challenge and want to get something more out of the course.\n",
        "\n",
        "Your task is to finetune the GPT2 model to generate lyrics for English songs. You can refer to [this blog](https://towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a) and follow the steps there. This blog shows how to finetune the GPT2 model to generate headlines for financial articles. Instead of headlines you will use lyrics so you may find the following datasets useful for training: [dataset1](https://data.world/datasets/lyrics), [dataset2](https://www.kaggle.com/paultimothymooney/poetry)\n",
        "\n",
        "At test time you will give it a prompt: \"I love Deep Learning\" and it should complete the song based on this prompt :-) Paste the generated song in a block below!"
      ],
      "metadata": {
        "id": "RvChaPTCbGIP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ccn5krpCcaEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Declaration\n",
        "\n",
        "\n",
        "\n",
        "I, Name_XXX (Roll no: XXYY), swear on my honour that I have written the code and the report by myself and have not copied it from the internet or other students.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HSPz9XJZbNh4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qzf9vWSyaiJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRnIpDOTaHjR"
      },
      "outputs": [],
      "source": []
    }
  ]
}